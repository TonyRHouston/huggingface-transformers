W0205 10:55:45.652000 2296508 torch/distributed/run.py:803] 
W0205 10:55:45.652000 2296508 torch/distributed/run.py:803] *****************************************
W0205 10:55:45.652000 2296508 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0205 10:55:45.652000 2296508 torch/distributed/run.py:803] *****************************************
Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]Loading weights:   0%|          | 1/291 [00:00<00:00, 11008.67it/s, Materializing param=lm_head.weight]Loading weights:   0%|          | 1/291 [00:00<00:00, 5698.78it/s, Materializing param=lm_head.weight] Loading weights:   1%|          | 2/291 [00:00<00:00, 5339.66it/s, Materializing param=model.embed_tokens.weight]Loading weights:   1%|          | 2/291 [00:00<00:00, 4596.50it/s, Materializing param=model.embed_tokens.weight]Loading weights:   1%|          | 3/291 [00:00<00:00, 4911.36it/s, Materializing param=model.layers.0.mlp.experts.gate_up_proj]Loading weights:   1%|          | 3/291 [00:00<00:00, 4396.55it/s, Materializing param=model.layers.0.mlp.experts.gate_up_proj]Loading weights:   1%|▏         | 4/291 [00:00<00:06, 41.06it/s, Materializing param=model.layers.0.mlp.experts.down_proj]     Loading weights:   1%|▏         | 4/291 [00:00<00:07, 40.90it/s, Materializing param=model.layers.0.mlp.experts.down_proj]Loading weights:   2%|▏         | 5/291 [00:00<00:05, 50.63it/s, Materializing param=model.layers.0.mlp.gate.weight]      Loading weights:   2%|▏         | 5/291 [00:00<00:05, 50.58it/s, Materializing param=model.layers.0.mlp.gate.weight]Loading weights:   2%|▏         | 6/291 [00:00<00:04, 60.52it/s, Materializing param=model.layers.0.input_layernorm.weight]Loading weights:   2%|▏         | 6/291 [00:00<00:04, 60.46it/s, Materializing param=model.layers.0.input_layernorm.weight]Loading weights:   2%|▏         | 7/291 [00:00<00:04, 70.30it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]Loading weights:   2%|▏         | 7/291 [00:00<00:04, 70.24it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]Loading weights:   3%|▎         | 8/291 [00:00<00:03, 80.15it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]        Loading weights:   3%|▎         | 8/291 [00:00<00:03, 80.09it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]Loading weights:   3%|▎         | 9/291 [00:00<00:03, 89.96it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]Loading weights:   3%|▎         | 9/291 [00:00<00:03, 89.96it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]Loading weights:   3%|▎         | 9/291 [00:00<00:03, 89.96it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]Loading weights:   3%|▎         | 10/291 [00:00<00:03, 89.96it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]Loading weights:   3%|▎         | 10/291 [00:00<00:03, 89.96it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]Loading weights:   4%|▍         | 11/291 [00:00<00:03, 89.96it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]Loading weights:   4%|▍         | 11/291 [00:00<00:03, 89.96it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]Loading weights:   4%|▍         | 12/291 [00:00<00:03, 89.96it/s, Materializing param=model.layers.1.mlp.experts.gate_up_proj]Loading weights:   4%|▍         | 12/291 [00:00<00:03, 89.96it/s, Materializing param=model.layers.1.mlp.experts.gate_up_proj]Loading weights:   4%|▍         | 13/291 [00:00<00:03, 89.96it/s, Materializing param=model.layers.1.mlp.experts.down_proj]   Loading weights:   4%|▍         | 13/291 [00:00<00:03, 89.96it/s, Materializing param=model.layers.1.mlp.experts.down_proj]Loading weights:   5%|▍         | 14/291 [00:00<00:03, 89.96it/s, Materializing param=model.layers.1.mlp.gate.weight]      Loading weights:   5%|▍         | 14/291 [00:00<00:03, 89.96it/s, Materializing param=model.layers.1.mlp.gate.weight]Loading weights:   5%|▌         | 15/291 [00:00<00:03, 89.96it/s, Materializing param=model.layers.1.input_layernorm.weight]Loading weights:   5%|▌         | 15/291 [00:00<00:03, 89.96it/s, Materializing param=model.layers.1.input_layernorm.weight]Loading weights:   5%|▌         | 16/291 [00:00<00:03, 89.96it/s, Materializing param=model.layers.1.post_attention_layernorm.weight]Loading weights:   5%|▌         | 16/291 [00:00<00:03, 89.96it/s, Materializing param=model.layers.1.post_attention_layernorm.weight]Loading weights:   6%|▌         | 17/291 [00:00<00:03, 89.96it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]        Loading weights:   6%|▌         | 17/291 [00:00<00:03, 89.96it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]Loading weights:   6%|▌         | 18/291 [00:00<00:03, 78.81it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]Loading weights:   6%|▌         | 18/291 [00:00<00:03, 78.81it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]Loading weights:   6%|▌         | 18/291 [00:00<00:03, 78.81it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]Loading weights:   7%|▋         | 19/291 [00:00<00:03, 78.81it/s, Materializing param=model.layers.1.self_attn.q_proj.weight]Loading weights:   7%|▋         | 19/291 [00:00<00:03, 78.81it/s, Materializing param=model.layers.1.self_attn.q_proj.weight]Loading weights:   7%|▋         | 20/291 [00:00<00:03, 78.81it/s, Materializing param=model.layers.1.self_attn.v_proj.weight]Loading weights:   7%|▋         | 20/291 [00:00<00:03, 78.81it/s, Materializing param=model.layers.1.self_attn.v_proj.weight]Loading weights:   7%|▋         | 21/291 [00:00<00:03, 78.81it/s, Materializing param=model.layers.2.mlp.experts.gate_up_proj]Loading weights:   7%|▋         | 21/291 [00:00<00:03, 78.81it/s, Materializing param=model.layers.2.mlp.experts.gate_up_proj]Loading weights:   8%|▊         | 22/291 [00:00<00:03, 78.81it/s, Materializing param=model.layers.2.mlp.experts.down_proj]   Loading weights:   8%|▊         | 22/291 [00:00<00:03, 78.81it/s, Materializing param=model.layers.2.mlp.experts.down_proj]Loading weights:   8%|▊         | 23/291 [00:00<00:03, 78.81it/s, Materializing param=model.layers.2.mlp.gate.weight]      Loading weights:   8%|▊         | 23/291 [00:00<00:03, 78.81it/s, Materializing param=model.layers.2.mlp.gate.weight]Loading weights:   8%|▊         | 24/291 [00:00<00:03, 78.81it/s, Materializing param=model.layers.2.input_layernorm.weight]Loading weights:   8%|▊         | 24/291 [00:00<00:03, 78.81it/s, Materializing param=model.layers.2.input_layernorm.weight]Loading weights:   9%|▊         | 25/291 [00:00<00:03, 78.81it/s, Materializing param=model.layers.2.post_attention_layernorm.weight]Loading weights:   9%|▊         | 25/291 [00:00<00:03, 78.81it/s, Materializing param=model.layers.2.post_attention_layernorm.weight]Loading weights:   9%|▉         | 26/291 [00:00<00:03, 72.03it/s, Materializing param=model.layers.2.post_attention_layernorm.weight]Loading weights:   9%|▉         | 26/291 [00:00<00:03, 72.03it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]        Loading weights:   9%|▉         | 26/291 [00:00<00:03, 72.03it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]Loading weights:   9%|▉         | 27/291 [00:00<00:03, 72.03it/s, Materializing param=model.layers.2.self_attn.o_proj.weight]Loading weights:   9%|▉         | 27/291 [00:00<00:03, 72.03it/s, Materializing param=model.layers.2.self_attn.o_proj.weight]Loading weights:  10%|▉         | 28/291 [00:00<00:03, 72.03it/s, Materializing param=model.layers.2.self_attn.q_proj.weight]Loading weights:  10%|▉         | 28/291 [00:00<00:03, 72.03it/s, Materializing param=model.layers.2.self_attn.q_proj.weight]Loading weights:  10%|▉         | 29/291 [00:00<00:03, 72.03it/s, Materializing param=model.layers.2.self_attn.v_proj.weight]Loading weights:  10%|▉         | 29/291 [00:00<00:03, 72.03it/s, Materializing param=model.layers.2.self_attn.v_proj.weight]Loading weights:  10%|█         | 30/291 [00:00<00:03, 72.03it/s, Materializing param=model.layers.3.mlp.experts.gate_up_proj]Loading weights:  10%|█         | 30/291 [00:00<00:03, 72.03it/s, Materializing param=model.layers.3.mlp.experts.gate_up_proj]Loading weights:  11%|█         | 31/291 [00:00<00:03, 72.03it/s, Materializing param=model.layers.3.mlp.experts.down_proj]   Loading weights:  11%|█         | 31/291 [00:00<00:03, 72.03it/s, Materializing param=model.layers.3.mlp.experts.down_proj]Loading weights:  11%|█         | 32/291 [00:00<00:03, 72.03it/s, Materializing param=model.layers.3.mlp.gate.weight]      Loading weights:  11%|█         | 32/291 [00:00<00:03, 72.03it/s, Materializing param=model.layers.3.mlp.gate.weight]Loading weights:  11%|█▏        | 33/291 [00:00<00:03, 72.03it/s, Materializing param=model.layers.3.input_layernorm.weight]Loading weights:  11%|█▏        | 33/291 [00:00<00:03, 72.03it/s, Materializing param=model.layers.3.input_layernorm.weight]Loading weights:  12%|█▏        | 34/291 [00:00<00:03, 72.03it/s, Materializing param=model.layers.3.post_attention_layernorm.weight]Loading weights:  12%|█▏        | 34/291 [00:00<00:03, 72.03it/s, Materializing param=model.layers.3.post_attention_layernorm.weight]Loading weights:  12%|█▏        | 35/291 [00:00<00:03, 72.03it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]        Loading weights:  12%|█▏        | 35/291 [00:00<00:03, 72.03it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]Loading weights:  12%|█▏        | 36/291 [00:00<00:03, 72.03it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]Loading weights:  12%|█▏        | 36/291 [00:00<00:03, 72.03it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]Loading weights:  13%|█▎        | 37/291 [00:00<00:03, 78.03it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]Loading weights:  13%|█▎        | 37/291 [00:00<00:03, 78.03it/s, Materializing param=model.layers.3.self_attn.q_proj.weight]Loading weights:  13%|█▎        | 37/291 [00:00<00:03, 78.03it/s, Materializing param=model.layers.3.self_attn.q_proj.weight]Loading weights:  13%|█▎        | 38/291 [00:00<00:03, 78.03it/s, Materializing param=model.layers.3.self_attn.v_proj.weight]Loading weights:  13%|█▎        | 38/291 [00:00<00:03, 78.03it/s, Materializing param=model.layers.3.self_attn.v_proj.weight]Loading weights:  13%|█▎        | 39/291 [00:00<00:03, 78.03it/s, Materializing param=model.layers.4.mlp.experts.gate_up_proj]Loading weights:  13%|█▎        | 39/291 [00:00<00:03, 78.03it/s, Materializing param=model.layers.4.mlp.experts.gate_up_proj]Loading weights:  14%|█▎        | 40/291 [00:00<00:03, 78.03it/s, Materializing param=model.layers.4.mlp.experts.down_proj]   Loading weights:  14%|█▎        | 40/291 [00:00<00:03, 78.03it/s, Materializing param=model.layers.4.mlp.experts.down_proj]Loading weights:  14%|█▍        | 41/291 [00:00<00:03, 78.03it/s, Materializing param=model.layers.4.mlp.gate.weight]      Loading weights:  14%|█▍        | 41/291 [00:00<00:03, 78.03it/s, Materializing param=model.layers.4.mlp.gate.weight]Loading weights:  14%|█▍        | 42/291 [00:00<00:03, 78.03it/s, Materializing param=model.layers.4.input_layernorm.weight]Loading weights:  14%|█▍        | 42/291 [00:00<00:03, 78.03it/s, Materializing param=model.layers.4.input_layernorm.weight]Loading weights:  15%|█▍        | 43/291 [00:00<00:03, 78.03it/s, Materializing param=model.layers.4.post_attention_layernorm.weight]Loading weights:  15%|█▍        | 43/291 [00:00<00:03, 78.03it/s, Materializing param=model.layers.4.post_attention_layernorm.weight]Loading weights:  15%|█▌        | 44/291 [00:00<00:03, 78.03it/s, Materializing param=model.layers.4.self_attn.k_proj.weight]        Loading weights:  15%|█▌        | 44/291 [00:00<00:03, 78.03it/s, Materializing param=model.layers.4.self_attn.k_proj.weight]Loading weights:  15%|█▌        | 45/291 [00:00<00:03, 78.03it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]Loading weights:  15%|█▌        | 45/291 [00:00<00:03, 78.03it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]Loading weights:  16%|█▌        | 46/291 [00:00<00:03, 76.84it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]Loading weights:  16%|█▌        | 46/291 [00:00<00:03, 76.84it/s, Materializing param=model.layers.4.self_attn.q_proj.weight]Loading weights:  16%|█▌        | 46/291 [00:00<00:03, 76.84it/s, Materializing param=model.layers.4.self_attn.q_proj.weight]Loading weights:  16%|█▌        | 47/291 [00:00<00:03, 76.84it/s, Materializing param=model.layers.4.self_attn.v_proj.weight]Loading weights:  16%|█▌        | 47/291 [00:00<00:03, 76.84it/s, Materializing param=model.layers.4.self_attn.v_proj.weight]Loading weights:  16%|█▋        | 48/291 [00:00<00:03, 76.84it/s, Materializing param=model.layers.5.mlp.experts.gate_up_proj]Loading weights:  16%|█▋        | 48/291 [00:00<00:03, 76.84it/s, Materializing param=model.layers.5.mlp.experts.gate_up_proj]Loading weights:  17%|█▋        | 49/291 [00:00<00:03, 76.84it/s, Materializing param=model.layers.5.mlp.experts.down_proj]   Loading weights:  17%|█▋        | 49/291 [00:00<00:03, 76.84it/s, Materializing param=model.layers.5.mlp.experts.down_proj]Loading weights:  17%|█▋        | 50/291 [00:00<00:03, 76.84it/s, Materializing param=model.layers.5.mlp.gate.weight]      Loading weights:  17%|█▋        | 50/291 [00:00<00:03, 76.84it/s, Materializing param=model.layers.5.mlp.gate.weight]Loading weights:  18%|█▊        | 51/291 [00:00<00:03, 76.84it/s, Materializing param=model.layers.5.input_layernorm.weight]Loading weights:  18%|█▊        | 51/291 [00:00<00:03, 76.84it/s, Materializing param=model.layers.5.input_layernorm.weight]Loading weights:  18%|█▊        | 52/291 [00:00<00:03, 76.84it/s, Materializing param=model.layers.5.post_attention_layernorm.weight]Loading weights:  18%|█▊        | 52/291 [00:00<00:03, 76.84it/s, Materializing param=model.layers.5.post_attention_layernorm.weight]Loading weights:  18%|█▊        | 53/291 [00:00<00:03, 76.84it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]        Loading weights:  18%|█▊        | 53/291 [00:00<00:03, 76.84it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]Loading weights:  19%|█▊        | 54/291 [00:00<00:03, 76.84it/s, Materializing param=model.layers.5.self_attn.o_proj.weight]Loading weights:  19%|█▊        | 54/291 [00:00<00:03, 76.84it/s, Materializing param=model.layers.5.self_attn.o_proj.weight]Loading weights:  19%|█▉        | 55/291 [00:00<00:02, 80.43it/s, Materializing param=model.layers.5.self_attn.o_proj.weight]Loading weights:  19%|█▉        | 55/291 [00:00<00:02, 80.43it/s, Materializing param=model.layers.5.self_attn.q_proj.weight]Loading weights:  19%|█▉        | 55/291 [00:00<00:02, 80.43it/s, Materializing param=model.layers.5.self_attn.q_proj.weight]Loading weights:  19%|█▉        | 56/291 [00:00<00:02, 80.43it/s, Materializing param=model.layers.5.self_attn.v_proj.weight]Loading weights:  19%|█▉        | 56/291 [00:00<00:02, 80.43it/s, Materializing param=model.layers.5.self_attn.v_proj.weight]Loading weights:  20%|█▉        | 57/291 [00:00<00:02, 80.43it/s, Materializing param=model.layers.6.mlp.experts.gate_up_proj]Loading weights:  20%|█▉        | 57/291 [00:00<00:02, 80.43it/s, Materializing param=model.layers.6.mlp.experts.gate_up_proj]Loading weights:  20%|█▉        | 58/291 [00:00<00:02, 80.43it/s, Materializing param=model.layers.6.mlp.experts.down_proj]   Loading weights:  20%|█▉        | 58/291 [00:00<00:02, 80.43it/s, Materializing param=model.layers.6.mlp.experts.down_proj]Loading weights:  20%|██        | 59/291 [00:00<00:02, 80.43it/s, Materializing param=model.layers.6.mlp.gate.weight]      Loading weights:  20%|██        | 59/291 [00:00<00:02, 80.43it/s, Materializing param=model.layers.6.mlp.gate.weight]Loading weights:  21%|██        | 60/291 [00:00<00:02, 80.43it/s, Materializing param=model.layers.6.input_layernorm.weight]Loading weights:  21%|██        | 60/291 [00:00<00:02, 80.43it/s, Materializing param=model.layers.6.input_layernorm.weight]Loading weights:  21%|██        | 61/291 [00:00<00:02, 80.43it/s, Materializing param=model.layers.6.post_attention_layernorm.weight]Loading weights:  21%|██        | 61/291 [00:00<00:02, 80.43it/s, Materializing param=model.layers.6.post_attention_layernorm.weight]Loading weights:  21%|██▏       | 62/291 [00:00<00:02, 80.43it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]        Loading weights:  21%|██▏       | 62/291 [00:00<00:02, 80.43it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]Loading weights:  22%|██▏       | 63/291 [00:00<00:02, 80.43it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]Loading weights:  22%|██▏       | 63/291 [00:00<00:02, 80.43it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]Loading weights:  22%|██▏       | 64/291 [00:00<00:02, 82.38it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]Loading weights:  22%|██▏       | 64/291 [00:00<00:02, 82.38it/s, Materializing param=model.layers.6.self_attn.q_proj.weight]Loading weights:  22%|██▏       | 64/291 [00:00<00:02, 82.38it/s, Materializing param=model.layers.6.self_attn.q_proj.weight]Loading weights:  22%|██▏       | 65/291 [00:00<00:02, 82.38it/s, Materializing param=model.layers.6.self_attn.v_proj.weight]Loading weights:  22%|██▏       | 65/291 [00:00<00:02, 82.38it/s, Materializing param=model.layers.6.self_attn.v_proj.weight]Loading weights:  23%|██▎       | 66/291 [00:00<00:02, 82.38it/s, Materializing param=model.layers.7.mlp.experts.gate_up_proj]Loading weights:  23%|██▎       | 66/291 [00:00<00:02, 82.38it/s, Materializing param=model.layers.7.mlp.experts.gate_up_proj]Loading weights:  23%|██▎       | 67/291 [00:00<00:02, 82.38it/s, Materializing param=model.layers.7.mlp.experts.down_proj]   Loading weights:  23%|██▎       | 67/291 [00:00<00:02, 82.38it/s, Materializing param=model.layers.7.mlp.experts.down_proj]Loading weights:  23%|██▎       | 68/291 [00:00<00:02, 82.38it/s, Materializing param=model.layers.7.mlp.gate.weight]      Loading weights:  23%|██▎       | 68/291 [00:00<00:02, 82.38it/s, Materializing param=model.layers.7.mlp.gate.weight]Loading weights:  24%|██▎       | 69/291 [00:00<00:02, 82.38it/s, Materializing param=model.layers.7.input_layernorm.weight]Loading weights:  24%|██▎       | 69/291 [00:00<00:02, 82.38it/s, Materializing param=model.layers.7.input_layernorm.weight]Loading weights:  24%|██▍       | 70/291 [00:00<00:02, 82.38it/s, Materializing param=model.layers.7.post_attention_layernorm.weight]Loading weights:  24%|██▍       | 70/291 [00:00<00:02, 82.38it/s, Materializing param=model.layers.7.post_attention_layernorm.weight]Loading weights:  24%|██▍       | 71/291 [00:00<00:02, 82.38it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]        Loading weights:  24%|██▍       | 71/291 [00:00<00:02, 82.38it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]Loading weights:  25%|██▍       | 72/291 [00:00<00:02, 82.38it/s, Materializing param=model.layers.7.self_attn.o_proj.weight]Loading weights:  25%|██▍       | 72/291 [00:00<00:02, 82.38it/s, Materializing param=model.layers.7.self_attn.o_proj.weight]Loading weights:  25%|██▌       | 73/291 [00:00<00:02, 82.38it/s, Materializing param=model.layers.7.self_attn.q_proj.weight]Loading weights:  25%|██▌       | 73/291 [00:00<00:02, 82.38it/s, Materializing param=model.layers.7.self_attn.q_proj.weight]Loading weights:  25%|██▌       | 74/291 [00:00<00:02, 82.38it/s, Materializing param=model.layers.7.self_attn.v_proj.weight]Loading weights:  25%|██▌       | 74/291 [00:00<00:02, 82.38it/s, Materializing param=model.layers.7.self_attn.v_proj.weight]Loading weights:  26%|██▌       | 75/291 [00:00<00:02, 82.38it/s, Materializing param=model.layers.8.mlp.experts.gate_up_proj]Loading weights:  26%|██▌       | 75/291 [00:00<00:02, 82.38it/s, Materializing param=model.layers.8.mlp.experts.gate_up_proj]Loading weights:  26%|██▌       | 76/291 [00:00<00:02, 80.39it/s, Materializing param=model.layers.8.mlp.experts.gate_up_proj]Loading weights:  26%|██▌       | 76/291 [00:00<00:02, 80.39it/s, Materializing param=model.layers.8.mlp.experts.down_proj]   Loading weights:  26%|██▌       | 76/291 [00:00<00:02, 80.39it/s, Materializing param=model.layers.8.mlp.experts.down_proj]Loading weights:  26%|██▋       | 77/291 [00:00<00:02, 80.39it/s, Materializing param=model.layers.8.mlp.gate.weight]      Loading weights:  26%|██▋       | 77/291 [00:00<00:02, 80.39it/s, Materializing param=model.layers.8.mlp.gate.weight]Loading weights:  27%|██▋       | 78/291 [00:00<00:02, 80.39it/s, Materializing param=model.layers.8.input_layernorm.weight]Loading weights:  27%|██▋       | 78/291 [00:00<00:02, 80.39it/s, Materializing param=model.layers.8.input_layernorm.weight]Loading weights:  27%|██▋       | 79/291 [00:00<00:02, 80.39it/s, Materializing param=model.layers.8.post_attention_layernorm.weight]Loading weights:  27%|██▋       | 79/291 [00:00<00:02, 80.39it/s, Materializing param=model.layers.8.post_attention_layernorm.weight]Loading weights:  27%|██▋       | 80/291 [00:00<00:02, 80.39it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]        Loading weights:  27%|██▋       | 80/291 [00:00<00:02, 80.39it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]Loading weights:  28%|██▊       | 81/291 [00:00<00:02, 80.39it/s, Materializing param=model.layers.8.self_attn.o_proj.weight]Loading weights:  28%|██▊       | 81/291 [00:00<00:02, 80.39it/s, Materializing param=model.layers.8.self_attn.o_proj.weight]Loading weights:  28%|██▊       | 82/291 [00:01<00:02, 80.39it/s, Materializing param=model.layers.8.self_attn.q_proj.weight]Loading weights:  28%|██▊       | 82/291 [00:01<00:02, 80.39it/s, Materializing param=model.layers.8.self_attn.q_proj.weight]Loading weights:  29%|██▊       | 83/291 [00:01<00:02, 80.39it/s, Materializing param=model.layers.8.self_attn.v_proj.weight]Loading weights:  29%|██▊       | 83/291 [00:01<00:02, 80.39it/s, Materializing param=model.layers.8.self_attn.v_proj.weight]Loading weights:  29%|██▉       | 84/291 [00:01<00:02, 80.39it/s, Materializing param=model.layers.9.mlp.experts.gate_up_proj]Loading weights:  29%|██▉       | 84/291 [00:01<00:02, 80.39it/s, Materializing param=model.layers.9.mlp.experts.gate_up_proj]Loading weights:  29%|██▉       | 85/291 [00:01<00:02, 82.20it/s, Materializing param=model.layers.9.mlp.experts.gate_up_proj]Loading weights:  29%|██▉       | 85/291 [00:01<00:02, 82.20it/s, Materializing param=model.layers.9.mlp.experts.down_proj]   Loading weights:  29%|██▉       | 85/291 [00:01<00:02, 82.20it/s, Materializing param=model.layers.9.mlp.experts.down_proj]Loading weights:  30%|██▉       | 86/291 [00:01<00:02, 82.20it/s, Materializing param=model.layers.9.mlp.gate.weight]      Loading weights:  30%|██▉       | 86/291 [00:01<00:02, 82.20it/s, Materializing param=model.layers.9.mlp.gate.weight]Loading weights:  30%|██▉       | 87/291 [00:01<00:02, 82.20it/s, Materializing param=model.layers.9.input_layernorm.weight]Loading weights:  30%|██▉       | 87/291 [00:01<00:02, 82.20it/s, Materializing param=model.layers.9.input_layernorm.weight]Loading weights:  30%|███       | 88/291 [00:01<00:02, 82.20it/s, Materializing param=model.layers.9.post_attention_layernorm.weight]Loading weights:  30%|███       | 88/291 [00:01<00:02, 82.20it/s, Materializing param=model.layers.9.post_attention_layernorm.weight]Loading weights:  31%|███       | 89/291 [00:01<00:02, 82.20it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]        Loading weights:  31%|███       | 89/291 [00:01<00:02, 82.20it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]Loading weights:  31%|███       | 90/291 [00:01<00:02, 82.20it/s, Materializing param=model.layers.9.self_attn.o_proj.weight]Loading weights:  31%|███       | 90/291 [00:01<00:02, 82.20it/s, Materializing param=model.layers.9.self_attn.o_proj.weight]Loading weights:  31%|███▏      | 91/291 [00:01<00:02, 82.20it/s, Materializing param=model.layers.9.self_attn.q_proj.weight]Loading weights:  31%|███▏      | 91/291 [00:01<00:02, 82.20it/s, Materializing param=model.layers.9.self_attn.q_proj.weight]Loading weights:  32%|███▏      | 92/291 [00:01<00:02, 82.20it/s, Materializing param=model.layers.9.self_attn.v_proj.weight]Loading weights:  32%|███▏      | 92/291 [00:01<00:02, 82.20it/s, Materializing param=model.layers.9.self_attn.v_proj.weight]Loading weights:  32%|███▏      | 93/291 [00:01<00:02, 82.20it/s, Materializing param=model.layers.10.mlp.experts.gate_up_proj]Loading weights:  32%|███▏      | 93/291 [00:01<00:02, 82.20it/s, Materializing param=model.layers.10.mlp.experts.gate_up_proj]Loading weights:  32%|███▏      | 94/291 [00:01<00:02, 79.84it/s, Materializing param=model.layers.10.mlp.experts.gate_up_proj]Loading weights:  32%|███▏      | 94/291 [00:01<00:02, 79.84it/s, Materializing param=model.layers.10.mlp.experts.down_proj]   Loading weights:  32%|███▏      | 94/291 [00:01<00:02, 79.84it/s, Materializing param=model.layers.10.mlp.experts.down_proj]Loading weights:  33%|███▎      | 95/291 [00:01<00:02, 79.84it/s, Materializing param=model.layers.10.mlp.gate.weight]      Loading weights:  33%|███▎      | 95/291 [00:01<00:02, 79.84it/s, Materializing param=model.layers.10.mlp.gate.weight]Loading weights:  33%|███▎      | 96/291 [00:01<00:02, 79.84it/s, Materializing param=model.layers.10.input_layernorm.weight]Loading weights:  33%|███▎      | 96/291 [00:01<00:02, 79.84it/s, Materializing param=model.layers.10.input_layernorm.weight]Loading weights:  33%|███▎      | 97/291 [00:01<00:02, 79.84it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]Loading weights:  33%|███▎      | 97/291 [00:01<00:02, 79.84it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]Loading weights:  34%|███▎      | 98/291 [00:01<00:02, 79.84it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]        Loading weights:  34%|███▎      | 98/291 [00:01<00:02, 79.84it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]Loading weights:  34%|███▍      | 99/291 [00:01<00:02, 79.84it/s, Materializing param=model.layers.10.self_attn.o_proj.weight]Loading weights:  34%|███▍      | 99/291 [00:01<00:02, 79.84it/s, Materializing param=model.layers.10.self_attn.o_proj.weight]Loading weights:  34%|███▍      | 100/291 [00:01<00:02, 79.84it/s, Materializing param=model.layers.10.self_attn.q_proj.weight]Loading weights:  34%|███▍      | 100/291 [00:01<00:02, 79.84it/s, Materializing param=model.layers.10.self_attn.q_proj.weight]Loading weights:  35%|███▍      | 101/291 [00:01<00:02, 79.84it/s, Materializing param=model.layers.10.self_attn.v_proj.weight]Loading weights:  35%|███▍      | 101/291 [00:01<00:02, 79.84it/s, Materializing param=model.layers.10.self_attn.v_proj.weight]Loading weights:  35%|███▌      | 102/291 [00:01<00:02, 79.84it/s, Materializing param=model.layers.11.mlp.experts.gate_up_proj]Loading weights:  35%|███▌      | 102/291 [00:01<00:02, 79.84it/s, Materializing param=model.layers.11.mlp.experts.gate_up_proj]Loading weights:  35%|███▌      | 103/291 [00:01<00:02, 80.76it/s, Materializing param=model.layers.11.mlp.experts.gate_up_proj]Loading weights:  35%|███▌      | 103/291 [00:01<00:02, 80.76it/s, Materializing param=model.layers.11.mlp.experts.down_proj]   Loading weights:  35%|███▌      | 103/291 [00:01<00:02, 80.76it/s, Materializing param=model.layers.11.mlp.experts.down_proj]Loading weights:  36%|███▌      | 104/291 [00:01<00:02, 80.76it/s, Materializing param=model.layers.11.mlp.gate.weight]      Loading weights:  36%|███▌      | 104/291 [00:01<00:02, 80.76it/s, Materializing param=model.layers.11.mlp.gate.weight]Loading weights:  36%|███▌      | 105/291 [00:01<00:02, 80.76it/s, Materializing param=model.layers.11.input_layernorm.weight]Loading weights:  36%|███▌      | 105/291 [00:01<00:02, 80.76it/s, Materializing param=model.layers.11.input_layernorm.weight]Loading weights:  36%|███▋      | 106/291 [00:01<00:02, 80.76it/s, Materializing param=model.layers.11.post_attention_layernorm.weight]Loading weights:  36%|███▋      | 106/291 [00:01<00:02, 80.76it/s, Materializing param=model.layers.11.post_attention_layernorm.weight]Loading weights:  37%|███▋      | 107/291 [00:01<00:02, 80.76it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]        Loading weights:  37%|███▋      | 107/291 [00:01<00:02, 80.76it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]Loading weights:  37%|███▋      | 108/291 [00:01<00:02, 80.76it/s, Materializing param=model.layers.11.self_attn.o_proj.weight]Loading weights:  37%|███▋      | 108/291 [00:01<00:02, 80.76it/s, Materializing param=model.layers.11.self_attn.o_proj.weight]Loading weights:  37%|███▋      | 109/291 [00:01<00:02, 80.76it/s, Materializing param=model.layers.11.self_attn.q_proj.weight]Loading weights:  37%|███▋      | 109/291 [00:01<00:02, 80.76it/s, Materializing param=model.layers.11.self_attn.q_proj.weight]Loading weights:  38%|███▊      | 110/291 [00:01<00:02, 80.76it/s, Materializing param=model.layers.11.self_attn.v_proj.weight]Loading weights:  38%|███▊      | 110/291 [00:01<00:02, 80.76it/s, Materializing param=model.layers.11.self_attn.v_proj.weight]Loading weights:  38%|███▊      | 111/291 [00:01<00:02, 80.76it/s, Materializing param=model.layers.12.mlp.experts.gate_up_proj]Loading weights:  38%|███▊      | 111/291 [00:01<00:02, 80.76it/s, Materializing param=model.layers.12.mlp.experts.gate_up_proj]Loading weights:  38%|███▊      | 112/291 [00:01<00:02, 81.32it/s, Materializing param=model.layers.12.mlp.experts.gate_up_proj]Loading weights:  38%|███▊      | 112/291 [00:01<00:02, 81.32it/s, Materializing param=model.layers.12.mlp.experts.down_proj]   Loading weights:  38%|███▊      | 112/291 [00:01<00:02, 81.32it/s, Materializing param=model.layers.12.mlp.experts.down_proj]Loading weights:  39%|███▉      | 113/291 [00:01<00:02, 81.32it/s, Materializing param=model.layers.12.mlp.gate.weight]      Loading weights:  39%|███▉      | 113/291 [00:01<00:02, 81.32it/s, Materializing param=model.layers.12.mlp.gate.weight]Loading weights:  39%|███▉      | 114/291 [00:01<00:02, 81.32it/s, Materializing param=model.layers.12.input_layernorm.weight]Loading weights:  39%|███▉      | 114/291 [00:01<00:02, 81.32it/s, Materializing param=model.layers.12.input_layernorm.weight]Loading weights:  40%|███▉      | 115/291 [00:01<00:02, 81.32it/s, Materializing param=model.layers.12.post_attention_layernorm.weight]Loading weights:  40%|███▉      | 115/291 [00:01<00:02, 81.32it/s, Materializing param=model.layers.12.post_attention_layernorm.weight]Loading weights:  40%|███▉      | 116/291 [00:01<00:02, 81.32it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]        Loading weights:  40%|███▉      | 116/291 [00:01<00:02, 81.32it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]Loading weights:  40%|████      | 117/291 [00:01<00:02, 81.32it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]Loading weights:  40%|████      | 117/291 [00:01<00:02, 81.32it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]Loading weights:  41%|████      | 118/291 [00:01<00:02, 81.32it/s, Materializing param=model.layers.12.self_attn.q_proj.weight]Loading weights:  41%|████      | 118/291 [00:01<00:02, 81.32it/s, Materializing param=model.layers.12.self_attn.q_proj.weight]Loading weights:  41%|████      | 119/291 [00:01<00:02, 81.32it/s, Materializing param=model.layers.12.self_attn.v_proj.weight]Loading weights:  41%|████      | 119/291 [00:01<00:02, 81.32it/s, Materializing param=model.layers.12.self_attn.v_proj.weight]Loading weights:  41%|████      | 120/291 [00:01<00:02, 81.32it/s, Materializing param=model.layers.13.mlp.experts.gate_up_proj]Loading weights:  41%|████      | 120/291 [00:01<00:02, 81.32it/s, Materializing param=model.layers.13.mlp.experts.gate_up_proj]Loading weights:  42%|████▏     | 121/291 [00:01<00:02, 80.66it/s, Materializing param=model.layers.13.mlp.experts.gate_up_proj]Loading weights:  42%|████▏     | 121/291 [00:01<00:02, 80.66it/s, Materializing param=model.layers.13.mlp.experts.down_proj]   Loading weights:  42%|████▏     | 121/291 [00:01<00:02, 80.66it/s, Materializing param=model.layers.13.mlp.experts.down_proj]Loading weights:  42%|████▏     | 122/291 [00:01<00:02, 80.66it/s, Materializing param=model.layers.13.mlp.gate.weight]      Loading weights:  42%|████▏     | 122/291 [00:01<00:02, 80.66it/s, Materializing param=model.layers.13.mlp.gate.weight]Loading weights:  42%|████▏     | 123/291 [00:01<00:02, 80.66it/s, Materializing param=model.layers.13.input_layernorm.weight]Loading weights:  42%|████▏     | 123/291 [00:01<00:02, 80.66it/s, Materializing param=model.layers.13.input_layernorm.weight]Loading weights:  43%|████▎     | 124/291 [00:01<00:02, 80.66it/s, Materializing param=model.layers.13.post_attention_layernorm.weight]Loading weights:  43%|████▎     | 124/291 [00:01<00:02, 80.66it/s, Materializing param=model.layers.13.post_attention_layernorm.weight]Loading weights:  43%|████▎     | 125/291 [00:01<00:02, 80.66it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]        Loading weights:  43%|████▎     | 125/291 [00:01<00:02, 80.66it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]Loading weights:  43%|████▎     | 126/291 [00:01<00:02, 80.66it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]Loading weights:  43%|████▎     | 126/291 [00:01<00:02, 80.66it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]Loading weights:  44%|████▎     | 127/291 [00:01<00:02, 80.66it/s, Materializing param=model.layers.13.self_attn.q_proj.weight]Loading weights:  44%|████▎     | 127/291 [00:01<00:02, 80.66it/s, Materializing param=model.layers.13.self_attn.q_proj.weight]Loading weights:  44%|████▍     | 128/291 [00:01<00:02, 80.66it/s, Materializing param=model.layers.13.self_attn.v_proj.weight]Loading weights:  44%|████▍     | 128/291 [00:01<00:02, 80.66it/s, Materializing param=model.layers.13.self_attn.v_proj.weight]Loading weights:  44%|████▍     | 129/291 [00:01<00:02, 80.66it/s, Materializing param=model.layers.14.mlp.experts.gate_up_proj]Loading weights:  44%|████▍     | 129/291 [00:01<00:02, 80.66it/s, Materializing param=model.layers.14.mlp.experts.gate_up_proj]Loading weights:  45%|████▍     | 130/291 [00:01<00:01, 81.09it/s, Materializing param=model.layers.14.mlp.experts.gate_up_proj]Loading weights:  45%|████▍     | 130/291 [00:01<00:01, 81.09it/s, Materializing param=model.layers.14.mlp.experts.down_proj]   Loading weights:  45%|████▍     | 130/291 [00:01<00:01, 81.09it/s, Materializing param=model.layers.14.mlp.experts.down_proj]Loading weights:  45%|████▌     | 131/291 [00:01<00:01, 81.09it/s, Materializing param=model.layers.14.mlp.gate.weight]      Loading weights:  45%|████▌     | 131/291 [00:01<00:01, 81.09it/s, Materializing param=model.layers.14.mlp.gate.weight]Loading weights:  45%|████▌     | 132/291 [00:01<00:01, 81.09it/s, Materializing param=model.layers.14.input_layernorm.weight]Loading weights:  45%|████▌     | 132/291 [00:01<00:01, 81.09it/s, Materializing param=model.layers.14.input_layernorm.weight]Loading weights:  46%|████▌     | 133/291 [00:01<00:01, 81.09it/s, Materializing param=model.layers.14.post_attention_layernorm.weight]Loading weights:  46%|████▌     | 133/291 [00:01<00:01, 81.09it/s, Materializing param=model.layers.14.post_attention_layernorm.weight]Loading weights:  46%|████▌     | 134/291 [00:01<00:01, 81.09it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]        Loading weights:  46%|████▌     | 134/291 [00:01<00:01, 81.09it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]Loading weights:  46%|████▋     | 135/291 [00:01<00:01, 81.09it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]Loading weights:  46%|████▋     | 135/291 [00:01<00:01, 81.09it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]Loading weights:  47%|████▋     | 136/291 [00:01<00:01, 81.09it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]Loading weights:  47%|████▋     | 136/291 [00:01<00:01, 81.09it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]Loading weights:  47%|████▋     | 137/291 [00:01<00:01, 81.09it/s, Materializing param=model.layers.14.self_attn.v_proj.weight]Loading weights:  47%|████▋     | 137/291 [00:01<00:01, 81.09it/s, Materializing param=model.layers.14.self_attn.v_proj.weight]Loading weights:  47%|████▋     | 138/291 [00:01<00:01, 81.09it/s, Materializing param=model.layers.15.mlp.experts.gate_up_proj]Loading weights:  47%|████▋     | 138/291 [00:01<00:01, 81.09it/s, Materializing param=model.layers.15.mlp.experts.gate_up_proj]Loading weights:  48%|████▊     | 139/291 [00:01<00:01, 81.69it/s, Materializing param=model.layers.15.mlp.experts.gate_up_proj]Loading weights:  48%|████▊     | 139/291 [00:01<00:01, 81.69it/s, Materializing param=model.layers.15.mlp.experts.down_proj]   Loading weights:  48%|████▊     | 139/291 [00:01<00:01, 81.69it/s, Materializing param=model.layers.15.mlp.experts.down_proj]Loading weights:  48%|████▊     | 140/291 [00:01<00:01, 81.69it/s, Materializing param=model.layers.15.mlp.gate.weight]      Loading weights:  48%|████▊     | 140/291 [00:01<00:01, 81.69it/s, Materializing param=model.layers.15.mlp.gate.weight]Loading weights:  48%|████▊     | 141/291 [00:01<00:01, 81.69it/s, Materializing param=model.layers.15.input_layernorm.weight]Loading weights:  48%|████▊     | 141/291 [00:01<00:01, 81.69it/s, Materializing param=model.layers.15.input_layernorm.weight]Loading weights:  49%|████▉     | 142/291 [00:01<00:01, 81.69it/s, Materializing param=model.layers.15.post_attention_layernorm.weight]Loading weights:  49%|████▉     | 142/291 [00:01<00:01, 81.69it/s, Materializing param=model.layers.15.post_attention_layernorm.weight]Loading weights:  49%|████▉     | 143/291 [00:01<00:01, 81.69it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]        Loading weights:  49%|████▉     | 143/291 [00:01<00:01, 81.69it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]Loading weights:  49%|████▉     | 144/291 [00:01<00:01, 81.69it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]Loading weights:  49%|████▉     | 144/291 [00:01<00:01, 81.69it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]Loading weights:  50%|████▉     | 145/291 [00:01<00:01, 81.69it/s, Materializing param=model.layers.15.self_attn.q_proj.weight]Loading weights:  50%|████▉     | 145/291 [00:01<00:01, 81.69it/s, Materializing param=model.layers.15.self_attn.q_proj.weight]Loading weights:  50%|█████     | 146/291 [00:01<00:01, 81.69it/s, Materializing param=model.layers.15.self_attn.v_proj.weight]Loading weights:  50%|█████     | 146/291 [00:01<00:01, 81.69it/s, Materializing param=model.layers.15.self_attn.v_proj.weight]Loading weights:  51%|█████     | 147/291 [00:01<00:01, 81.69it/s, Materializing param=model.layers.16.mlp.experts.gate_up_proj]Loading weights:  51%|█████     | 147/291 [00:01<00:01, 81.69it/s, Materializing param=model.layers.16.mlp.experts.gate_up_proj]Loading weights:  51%|█████     | 148/291 [00:01<00:01, 82.29it/s, Materializing param=model.layers.16.mlp.experts.gate_up_proj]Loading weights:  51%|█████     | 148/291 [00:01<00:01, 82.29it/s, Materializing param=model.layers.16.mlp.experts.down_proj]   Loading weights:  51%|█████     | 148/291 [00:01<00:01, 82.29it/s, Materializing param=model.layers.16.mlp.experts.down_proj]Loading weights:  51%|█████     | 149/291 [00:01<00:01, 82.29it/s, Materializing param=model.layers.16.mlp.gate.weight]      Loading weights:  51%|█████     | 149/291 [00:01<00:01, 82.29it/s, Materializing param=model.layers.16.mlp.gate.weight]Loading weights:  52%|█████▏    | 150/291 [00:01<00:01, 82.29it/s, Materializing param=model.layers.16.input_layernorm.weight]Loading weights:  52%|█████▏    | 150/291 [00:01<00:01, 82.29it/s, Materializing param=model.layers.16.input_layernorm.weight]Loading weights:  52%|█████▏    | 151/291 [00:01<00:01, 82.29it/s, Materializing param=model.layers.16.post_attention_layernorm.weight]Loading weights:  52%|█████▏    | 151/291 [00:01<00:01, 82.29it/s, Materializing param=model.layers.16.post_attention_layernorm.weight]Loading weights:  52%|█████▏    | 152/291 [00:01<00:01, 82.29it/s, Materializing param=model.layers.16.self_attn.k_proj.weight]        Loading weights:  52%|█████▏    | 152/291 [00:01<00:01, 82.29it/s, Materializing param=model.layers.16.self_attn.k_proj.weight]Loading weights:  53%|█████▎    | 153/291 [00:01<00:01, 82.29it/s, Materializing param=model.layers.16.self_attn.o_proj.weight]Loading weights:  53%|█████▎    | 153/291 [00:01<00:01, 82.29it/s, Materializing param=model.layers.16.self_attn.o_proj.weight]Loading weights:  53%|█████▎    | 154/291 [00:01<00:01, 82.29it/s, Materializing param=model.layers.16.self_attn.q_proj.weight]Loading weights:  53%|█████▎    | 154/291 [00:01<00:01, 82.29it/s, Materializing param=model.layers.16.self_attn.q_proj.weight]Loading weights:  53%|█████▎    | 155/291 [00:01<00:01, 82.29it/s, Materializing param=model.layers.16.self_attn.v_proj.weight]Loading weights:  53%|█████▎    | 155/291 [00:01<00:01, 82.29it/s, Materializing param=model.layers.16.self_attn.v_proj.weight]Loading weights:  54%|█████▎    | 156/291 [00:01<00:01, 82.29it/s, Materializing param=model.layers.17.mlp.experts.gate_up_proj]Loading weights:  54%|█████▎    | 156/291 [00:01<00:01, 82.29it/s, Materializing param=model.layers.17.mlp.experts.gate_up_proj]Loading weights:  54%|█████▍    | 157/291 [00:01<00:01, 82.64it/s, Materializing param=model.layers.17.mlp.experts.gate_up_proj]Loading weights:  54%|█████▍    | 157/291 [00:01<00:01, 82.64it/s, Materializing param=model.layers.17.mlp.experts.down_proj]   Loading weights:  54%|█████▍    | 157/291 [00:01<00:01, 82.64it/s, Materializing param=model.layers.17.mlp.experts.down_proj]Loading weights:  54%|█████▍    | 158/291 [00:01<00:01, 82.64it/s, Materializing param=model.layers.17.mlp.gate.weight]      Loading weights:  54%|█████▍    | 158/291 [00:01<00:01, 82.64it/s, Materializing param=model.layers.17.mlp.gate.weight]Loading weights:  55%|█████▍    | 159/291 [00:01<00:01, 82.64it/s, Materializing param=model.layers.17.input_layernorm.weight]Loading weights:  55%|█████▍    | 159/291 [00:01<00:01, 82.64it/s, Materializing param=model.layers.17.input_layernorm.weight]Loading weights:  55%|█████▍    | 160/291 [00:01<00:01, 82.64it/s, Materializing param=model.layers.17.post_attention_layernorm.weight]Loading weights:  55%|█████▍    | 160/291 [00:01<00:01, 82.64it/s, Materializing param=model.layers.17.post_attention_layernorm.weight]Loading weights:  55%|█████▌    | 161/291 [00:01<00:01, 82.64it/s, Materializing param=model.layers.17.self_attn.k_proj.weight]        Loading weights:  55%|█████▌    | 161/291 [00:01<00:01, 82.64it/s, Materializing param=model.layers.17.self_attn.k_proj.weight]Loading weights:  56%|█████▌    | 162/291 [00:01<00:01, 82.64it/s, Materializing param=model.layers.17.self_attn.o_proj.weight]Loading weights:  56%|█████▌    | 162/291 [00:01<00:01, 82.64it/s, Materializing param=model.layers.17.self_attn.o_proj.weight]Loading weights:  56%|█████▌    | 163/291 [00:01<00:01, 82.64it/s, Materializing param=model.layers.17.self_attn.q_proj.weight]Loading weights:  56%|█████▌    | 163/291 [00:01<00:01, 82.64it/s, Materializing param=model.layers.17.self_attn.q_proj.weight]Loading weights:  56%|█████▋    | 164/291 [00:01<00:01, 82.64it/s, Materializing param=model.layers.17.self_attn.v_proj.weight]Loading weights:  56%|█████▋    | 164/291 [00:01<00:01, 82.64it/s, Materializing param=model.layers.17.self_attn.v_proj.weight]Loading weights:  57%|█████▋    | 165/291 [00:01<00:01, 82.64it/s, Materializing param=model.layers.18.mlp.experts.gate_up_proj]Loading weights:  57%|█████▋    | 165/291 [00:01<00:01, 82.64it/s, Materializing param=model.layers.18.mlp.experts.gate_up_proj]Loading weights:  57%|█████▋    | 166/291 [00:02<00:01, 82.77it/s, Materializing param=model.layers.18.mlp.experts.gate_up_proj]Loading weights:  57%|█████▋    | 166/291 [00:02<00:01, 82.77it/s, Materializing param=model.layers.18.mlp.experts.down_proj]   Loading weights:  57%|█████▋    | 166/291 [00:02<00:01, 82.77it/s, Materializing param=model.layers.18.mlp.experts.down_proj]Loading weights:  57%|█████▋    | 167/291 [00:02<00:01, 82.77it/s, Materializing param=model.layers.18.mlp.gate.weight]      Loading weights:  57%|█████▋    | 167/291 [00:02<00:01, 82.77it/s, Materializing param=model.layers.18.mlp.gate.weight]Loading weights:  58%|█████▊    | 168/291 [00:02<00:01, 82.77it/s, Materializing param=model.layers.18.input_layernorm.weight]Loading weights:  58%|█████▊    | 168/291 [00:02<00:01, 82.77it/s, Materializing param=model.layers.18.input_layernorm.weight]Loading weights:  58%|█████▊    | 169/291 [00:02<00:01, 82.77it/s, Materializing param=model.layers.18.post_attention_layernorm.weight]Loading weights:  58%|█████▊    | 169/291 [00:02<00:01, 82.77it/s, Materializing param=model.layers.18.post_attention_layernorm.weight]Loading weights:  58%|█████▊    | 170/291 [00:02<00:01, 82.77it/s, Materializing param=model.layers.18.self_attn.k_proj.weight]        Loading weights:  58%|█████▊    | 170/291 [00:02<00:01, 82.77it/s, Materializing param=model.layers.18.self_attn.k_proj.weight]Loading weights:  59%|█████▉    | 171/291 [00:02<00:01, 82.77it/s, Materializing param=model.layers.18.self_attn.o_proj.weight]Loading weights:  59%|█████▉    | 171/291 [00:02<00:01, 82.77it/s, Materializing param=model.layers.18.self_attn.o_proj.weight]Loading weights:  59%|█████▉    | 172/291 [00:02<00:01, 82.77it/s, Materializing param=model.layers.18.self_attn.q_proj.weight]Loading weights:  59%|█████▉    | 172/291 [00:02<00:01, 82.77it/s, Materializing param=model.layers.18.self_attn.q_proj.weight]Loading weights:  59%|█████▉    | 173/291 [00:02<00:01, 82.77it/s, Materializing param=model.layers.18.self_attn.v_proj.weight]Loading weights:  59%|█████▉    | 173/291 [00:02<00:01, 82.77it/s, Materializing param=model.layers.18.self_attn.v_proj.weight]Loading weights:  60%|█████▉    | 174/291 [00:02<00:01, 82.77it/s, Materializing param=model.layers.19.mlp.experts.gate_up_proj]Loading weights:  60%|█████▉    | 174/291 [00:02<00:01, 82.77it/s, Materializing param=model.layers.19.mlp.experts.gate_up_proj]Loading weights:  60%|██████    | 175/291 [00:02<00:01, 81.34it/s, Materializing param=model.layers.19.mlp.experts.gate_up_proj]Loading weights:  60%|██████    | 175/291 [00:02<00:01, 81.34it/s, Materializing param=model.layers.19.mlp.experts.down_proj]   Loading weights:  60%|██████    | 175/291 [00:02<00:01, 81.34it/s, Materializing param=model.layers.19.mlp.experts.down_proj]Loading weights:  60%|██████    | 176/291 [00:02<00:01, 81.34it/s, Materializing param=model.layers.19.mlp.gate.weight]      Loading weights:  60%|██████    | 176/291 [00:02<00:01, 81.34it/s, Materializing param=model.layers.19.mlp.gate.weight]Loading weights:  61%|██████    | 177/291 [00:02<00:01, 81.34it/s, Materializing param=model.layers.19.input_layernorm.weight]Loading weights:  61%|██████    | 177/291 [00:02<00:01, 81.34it/s, Materializing param=model.layers.19.input_layernorm.weight]Loading weights:  61%|██████    | 178/291 [00:02<00:01, 81.34it/s, Materializing param=model.layers.19.post_attention_layernorm.weight]Loading weights:  61%|██████    | 178/291 [00:02<00:01, 81.34it/s, Materializing param=model.layers.19.post_attention_layernorm.weight]Loading weights:  62%|██████▏   | 179/291 [00:02<00:01, 81.34it/s, Materializing param=model.layers.19.self_attn.k_proj.weight]        Loading weights:  62%|██████▏   | 179/291 [00:02<00:01, 81.34it/s, Materializing param=model.layers.19.self_attn.k_proj.weight]Loading weights:  62%|██████▏   | 180/291 [00:02<00:01, 81.34it/s, Materializing param=model.layers.19.self_attn.o_proj.weight]Loading weights:  62%|██████▏   | 180/291 [00:02<00:01, 81.34it/s, Materializing param=model.layers.19.self_attn.o_proj.weight]Loading weights:  62%|██████▏   | 181/291 [00:02<00:01, 81.34it/s, Materializing param=model.layers.19.self_attn.q_proj.weight]Loading weights:  62%|██████▏   | 181/291 [00:02<00:01, 81.34it/s, Materializing param=model.layers.19.self_attn.q_proj.weight]Loading weights:  63%|██████▎   | 182/291 [00:02<00:01, 81.34it/s, Materializing param=model.layers.19.self_attn.v_proj.weight]Loading weights:  63%|██████▎   | 182/291 [00:02<00:01, 81.34it/s, Materializing param=model.layers.19.self_attn.v_proj.weight]Loading weights:  63%|██████▎   | 183/291 [00:02<00:01, 81.34it/s, Materializing param=model.layers.20.mlp.experts.gate_up_proj]Loading weights:  63%|██████▎   | 183/291 [00:02<00:01, 81.34it/s, Materializing param=model.layers.20.mlp.experts.gate_up_proj]Loading weights:  63%|██████▎   | 184/291 [00:02<00:01, 82.12it/s, Materializing param=model.layers.20.mlp.experts.gate_up_proj]Loading weights:  63%|██████▎   | 184/291 [00:02<00:01, 82.12it/s, Materializing param=model.layers.20.mlp.experts.down_proj]   Loading weights:  63%|██████▎   | 184/291 [00:02<00:01, 82.12it/s, Materializing param=model.layers.20.mlp.experts.down_proj]Loading weights:  64%|██████▎   | 185/291 [00:02<00:01, 82.12it/s, Materializing param=model.layers.20.mlp.gate.weight]      Loading weights:  64%|██████▎   | 185/291 [00:02<00:01, 82.12it/s, Materializing param=model.layers.20.mlp.gate.weight]Loading weights:  64%|██████▍   | 186/291 [00:02<00:01, 82.12it/s, Materializing param=model.layers.20.input_layernorm.weight]Loading weights:  64%|██████▍   | 186/291 [00:02<00:01, 82.12it/s, Materializing param=model.layers.20.input_layernorm.weight]Loading weights:  64%|██████▍   | 187/291 [00:02<00:01, 82.12it/s, Materializing param=model.layers.20.post_attention_layernorm.weight]Loading weights:  64%|██████▍   | 187/291 [00:02<00:01, 82.12it/s, Materializing param=model.layers.20.post_attention_layernorm.weight]Loading weights:  65%|██████▍   | 188/291 [00:02<00:01, 82.12it/s, Materializing param=model.layers.20.self_attn.k_proj.weight]        Loading weights:  65%|██████▍   | 188/291 [00:02<00:01, 82.12it/s, Materializing param=model.layers.20.self_attn.k_proj.weight]Loading weights:  65%|██████▍   | 189/291 [00:02<00:01, 82.12it/s, Materializing param=model.layers.20.self_attn.o_proj.weight]Loading weights:  65%|██████▍   | 189/291 [00:02<00:01, 82.12it/s, Materializing param=model.layers.20.self_attn.o_proj.weight]Loading weights:  65%|██████▌   | 190/291 [00:02<00:01, 82.12it/s, Materializing param=model.layers.20.self_attn.q_proj.weight]Loading weights:  65%|██████▌   | 190/291 [00:02<00:01, 82.12it/s, Materializing param=model.layers.20.self_attn.q_proj.weight]Loading weights:  66%|██████▌   | 191/291 [00:02<00:01, 82.12it/s, Materializing param=model.layers.20.self_attn.v_proj.weight]Loading weights:  66%|██████▌   | 191/291 [00:02<00:01, 82.12it/s, Materializing param=model.layers.20.self_attn.v_proj.weight]Loading weights:  66%|██████▌   | 192/291 [00:02<00:01, 82.12it/s, Materializing param=model.layers.21.mlp.experts.gate_up_proj]Loading weights:  66%|██████▌   | 192/291 [00:02<00:01, 82.12it/s, Materializing param=model.layers.21.mlp.experts.gate_up_proj]Loading weights:  66%|██████▋   | 193/291 [00:02<00:01, 83.35it/s, Materializing param=model.layers.21.mlp.experts.gate_up_proj]Loading weights:  66%|██████▋   | 193/291 [00:02<00:01, 83.35it/s, Materializing param=model.layers.21.mlp.experts.down_proj]   Loading weights:  66%|██████▋   | 193/291 [00:02<00:01, 83.35it/s, Materializing param=model.layers.21.mlp.experts.down_proj]Loading weights:  67%|██████▋   | 194/291 [00:02<00:01, 83.35it/s, Materializing param=model.layers.21.mlp.gate.weight]      Loading weights:  67%|██████▋   | 194/291 [00:02<00:01, 83.35it/s, Materializing param=model.layers.21.mlp.gate.weight]Loading weights:  67%|██████▋   | 195/291 [00:02<00:01, 83.35it/s, Materializing param=model.layers.21.input_layernorm.weight]Loading weights:  67%|██████▋   | 195/291 [00:02<00:01, 83.35it/s, Materializing param=model.layers.21.input_layernorm.weight]Loading weights:  67%|██████▋   | 196/291 [00:02<00:01, 83.35it/s, Materializing param=model.layers.21.post_attention_layernorm.weight]Loading weights:  67%|██████▋   | 196/291 [00:02<00:01, 83.35it/s, Materializing param=model.layers.21.post_attention_layernorm.weight]Loading weights:  68%|██████▊   | 197/291 [00:02<00:01, 83.35it/s, Materializing param=model.layers.21.self_attn.k_proj.weight]        Loading weights:  68%|██████▊   | 197/291 [00:02<00:01, 83.35it/s, Materializing param=model.layers.21.self_attn.k_proj.weight]Loading weights:  68%|██████▊   | 198/291 [00:02<00:01, 83.35it/s, Materializing param=model.layers.21.self_attn.o_proj.weight]Loading weights:  68%|██████▊   | 198/291 [00:02<00:01, 83.35it/s, Materializing param=model.layers.21.self_attn.o_proj.weight]Loading weights:  68%|██████▊   | 199/291 [00:02<00:01, 83.35it/s, Materializing param=model.layers.21.self_attn.q_proj.weight]Loading weights:  68%|██████▊   | 199/291 [00:02<00:01, 83.35it/s, Materializing param=model.layers.21.self_attn.q_proj.weight]Loading weights:  69%|██████▊   | 200/291 [00:02<00:01, 83.35it/s, Materializing param=model.layers.21.self_attn.v_proj.weight]Loading weights:  69%|██████▊   | 200/291 [00:02<00:01, 83.35it/s, Materializing param=model.layers.21.self_attn.v_proj.weight]Loading weights:  69%|██████▉   | 201/291 [00:02<00:01, 83.35it/s, Materializing param=model.layers.22.mlp.experts.gate_up_proj]Loading weights:  69%|██████▉   | 201/291 [00:02<00:01, 83.35it/s, Materializing param=model.layers.22.mlp.experts.gate_up_proj]Loading weights:  69%|██████▉   | 202/291 [00:02<00:01, 84.87it/s, Materializing param=model.layers.22.mlp.experts.gate_up_proj]Loading weights:  69%|██████▉   | 202/291 [00:02<00:01, 84.87it/s, Materializing param=model.layers.22.mlp.experts.down_proj]   Loading weights:  69%|██████▉   | 202/291 [00:02<00:01, 84.87it/s, Materializing param=model.layers.22.mlp.experts.down_proj]Loading weights:  70%|██████▉   | 203/291 [00:02<00:01, 84.87it/s, Materializing param=model.layers.22.mlp.gate.weight]      Loading weights:  70%|██████▉   | 203/291 [00:02<00:01, 84.87it/s, Materializing param=model.layers.22.mlp.gate.weight]Loading weights:  70%|███████   | 204/291 [00:02<00:01, 84.87it/s, Materializing param=model.layers.22.input_layernorm.weight]Loading weights:  70%|███████   | 204/291 [00:02<00:01, 84.87it/s, Materializing param=model.layers.22.input_layernorm.weight]Loading weights:  70%|███████   | 205/291 [00:02<00:01, 84.87it/s, Materializing param=model.layers.22.post_attention_layernorm.weight]Loading weights:  70%|███████   | 205/291 [00:02<00:01, 84.87it/s, Materializing param=model.layers.22.post_attention_layernorm.weight]Loading weights:  71%|███████   | 206/291 [00:02<00:01, 84.87it/s, Materializing param=model.layers.22.self_attn.k_proj.weight]        Loading weights:  71%|███████   | 206/291 [00:02<00:01, 84.87it/s, Materializing param=model.layers.22.self_attn.k_proj.weight]Loading weights:  71%|███████   | 207/291 [00:02<00:00, 84.87it/s, Materializing param=model.layers.22.self_attn.o_proj.weight]Loading weights:  71%|███████   | 207/291 [00:02<00:00, 84.87it/s, Materializing param=model.layers.22.self_attn.o_proj.weight]Loading weights:  71%|███████▏  | 208/291 [00:02<00:00, 84.87it/s, Materializing param=model.layers.22.self_attn.q_proj.weight]Loading weights:  71%|███████▏  | 208/291 [00:02<00:00, 84.87it/s, Materializing param=model.layers.22.self_attn.q_proj.weight]Loading weights:  72%|███████▏  | 209/291 [00:02<00:00, 84.87it/s, Materializing param=model.layers.22.self_attn.v_proj.weight]Loading weights:  72%|███████▏  | 209/291 [00:02<00:00, 84.87it/s, Materializing param=model.layers.22.self_attn.v_proj.weight]Loading weights:  72%|███████▏  | 210/291 [00:02<00:00, 84.87it/s, Materializing param=model.layers.23.mlp.experts.gate_up_proj]Loading weights:  72%|███████▏  | 210/291 [00:02<00:00, 84.87it/s, Materializing param=model.layers.23.mlp.experts.gate_up_proj]Loading weights:  73%|███████▎  | 211/291 [00:02<00:00, 83.37it/s, Materializing param=model.layers.23.mlp.experts.gate_up_proj]Loading weights:  73%|███████▎  | 211/291 [00:02<00:00, 83.37it/s, Materializing param=model.layers.23.mlp.experts.down_proj]   Loading weights:  73%|███████▎  | 211/291 [00:02<00:00, 83.37it/s, Materializing param=model.layers.23.mlp.experts.down_proj]Loading weights:  73%|███████▎  | 212/291 [00:02<00:00, 83.37it/s, Materializing param=model.layers.23.mlp.gate.weight]      Loading weights:  73%|███████▎  | 212/291 [00:02<00:00, 83.37it/s, Materializing param=model.layers.23.mlp.gate.weight]Loading weights:  73%|███████▎  | 213/291 [00:02<00:00, 83.37it/s, Materializing param=model.layers.23.input_layernorm.weight]Loading weights:  73%|███████▎  | 213/291 [00:02<00:00, 83.37it/s, Materializing param=model.layers.23.input_layernorm.weight]Loading weights:  74%|███████▎  | 214/291 [00:02<00:00, 83.37it/s, Materializing param=model.layers.23.post_attention_layernorm.weight]Loading weights:  74%|███████▎  | 214/291 [00:02<00:00, 83.37it/s, Materializing param=model.layers.23.post_attention_layernorm.weight]Loading weights:  74%|███████▍  | 215/291 [00:02<00:00, 83.37it/s, Materializing param=model.layers.23.self_attn.k_proj.weight]        Loading weights:  74%|███████▍  | 215/291 [00:02<00:00, 83.37it/s, Materializing param=model.layers.23.self_attn.k_proj.weight]Loading weights:  74%|███████▍  | 216/291 [00:02<00:00, 83.37it/s, Materializing param=model.layers.23.self_attn.o_proj.weight]Loading weights:  74%|███████▍  | 216/291 [00:02<00:00, 83.37it/s, Materializing param=model.layers.23.self_attn.o_proj.weight]Loading weights:  75%|███████▍  | 217/291 [00:02<00:00, 83.37it/s, Materializing param=model.layers.23.self_attn.q_proj.weight]Loading weights:  75%|███████▍  | 217/291 [00:02<00:00, 83.37it/s, Materializing param=model.layers.23.self_attn.q_proj.weight]Loading weights:  75%|███████▍  | 218/291 [00:02<00:00, 83.37it/s, Materializing param=model.layers.23.self_attn.v_proj.weight]Loading weights:  75%|███████▍  | 218/291 [00:02<00:00, 83.37it/s, Materializing param=model.layers.23.self_attn.v_proj.weight]Loading weights:  75%|███████▌  | 219/291 [00:02<00:00, 83.37it/s, Materializing param=model.layers.24.mlp.experts.gate_up_proj]Loading weights:  75%|███████▌  | 219/291 [00:02<00:00, 83.37it/s, Materializing param=model.layers.24.mlp.experts.gate_up_proj]Loading weights:  76%|███████▌  | 220/291 [00:02<00:00, 84.17it/s, Materializing param=model.layers.24.mlp.experts.gate_up_proj]Loading weights:  76%|███████▌  | 220/291 [00:02<00:00, 84.17it/s, Materializing param=model.layers.24.mlp.experts.down_proj]   Loading weights:  76%|███████▌  | 220/291 [00:02<00:00, 84.17it/s, Materializing param=model.layers.24.mlp.experts.down_proj]Loading weights:  76%|███████▌  | 221/291 [00:02<00:00, 84.17it/s, Materializing param=model.layers.24.mlp.gate.weight]      Loading weights:  76%|███████▌  | 221/291 [00:02<00:00, 84.17it/s, Materializing param=model.layers.24.mlp.gate.weight]Loading weights:  76%|███████▋  | 222/291 [00:02<00:00, 84.17it/s, Materializing param=model.layers.24.input_layernorm.weight]Loading weights:  76%|███████▋  | 222/291 [00:02<00:00, 84.17it/s, Materializing param=model.layers.24.input_layernorm.weight]Loading weights:  77%|███████▋  | 223/291 [00:02<00:00, 84.17it/s, Materializing param=model.layers.24.post_attention_layernorm.weight]Loading weights:  77%|███████▋  | 223/291 [00:02<00:00, 84.17it/s, Materializing param=model.layers.24.post_attention_layernorm.weight]Loading weights:  77%|███████▋  | 224/291 [00:02<00:00, 84.17it/s, Materializing param=model.layers.24.self_attn.k_proj.weight]        Loading weights:  77%|███████▋  | 224/291 [00:02<00:00, 84.17it/s, Materializing param=model.layers.24.self_attn.k_proj.weight]Loading weights:  77%|███████▋  | 225/291 [00:02<00:00, 84.17it/s, Materializing param=model.layers.24.self_attn.o_proj.weight]Loading weights:  77%|███████▋  | 225/291 [00:02<00:00, 84.17it/s, Materializing param=model.layers.24.self_attn.o_proj.weight]Loading weights:  78%|███████▊  | 226/291 [00:02<00:00, 84.17it/s, Materializing param=model.layers.24.self_attn.q_proj.weight]Loading weights:  78%|███████▊  | 226/291 [00:02<00:00, 84.17it/s, Materializing param=model.layers.24.self_attn.q_proj.weight]Loading weights:  78%|███████▊  | 227/291 [00:02<00:00, 84.17it/s, Materializing param=model.layers.24.self_attn.v_proj.weight]Loading weights:  78%|███████▊  | 227/291 [00:02<00:00, 84.17it/s, Materializing param=model.layers.24.self_attn.v_proj.weight]Loading weights:  78%|███████▊  | 228/291 [00:02<00:00, 84.17it/s, Materializing param=model.layers.25.mlp.experts.gate_up_proj]Loading weights:  78%|███████▊  | 228/291 [00:02<00:00, 84.17it/s, Materializing param=model.layers.25.mlp.experts.gate_up_proj][1mMixtralForCausalLM LOAD REPORT[0m from: mistralai/Mixtral-8x7B-Instruct-v0.1
Key                                            | Status     |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      
-----------------------------------------------+------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
model.layers.{0...31}.mlp.experts.down_proj    | MISSING    |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      
model.layers.{0...31}.mlp.experts.gate_up_proj | MISSING    |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      
model.layers.{0...31}.mlp.experts.gate_up_proj | CONVERSION | 

Traceback (most recent call last):
  File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/work/fix-moe-ep/src/transformers/core_model_loading.py", line 832, in log_conversion_errors
    yield
  File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/work/fix-moe-ep/src/transformers/core_model_loading.py", line 724, in convert
    collected_tensors = op.convert(
                        ^^^^^^^^^^^
  File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/env_main/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/work/fix-moe-ep/src/transformers/core_model_loading.py", line 188, in convert
    merged[target_pattern] = torch.stack([k for k in tensors if k != []], dim=self.dim)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: stack expects each tensor to be equal size, but got [] at entry 0 and [14336, 4096] at entry 2
stack expects each tensor to be equal size, but got [] at entry 0 and [14336, 4096] at entry 2
Error: MergeModulelist on tensors destined for model.layers.31.mlp.experts.gate_up_proj. Ckpt contains: 2


model.layers.{0...31}.mlp.experts.down_proj    | CONVERSION | 

Traceback (most recent call last):
  File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/work/fix-moe-ep/src/transformers/core_model_loading.py", line 832, in log_conversion_errors
    yield
  File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/work/fix-moe-ep/src/transformers/core_model_loading.py", line 724, in convert
    collected_tensors = op.convert(
                        ^^^^^^^^^^^
  File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/env_main/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/work/fix-moe-ep/src/transformers/core_model_loading.py", line 188, in convert
    merged[target_pattern] = torch.stack([k for k in tensors if k != []], dim=self.dim)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: stack expects each tensor to be equal size, but got [] at entry 0 and [4096, 14336] at entry 2
stack expects each tensor to be equal size, but got [] at entry 0 and [4096, 14336] at entry 2
Error: MergeModulelist on tensors destined for model.layers.31.mlp.experts.down_proj. Ckpt contains: 1

   

[3mNotes:
- MISSING[3m	:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.
- CONVERSION[3m	:originate from the conversion scheme[0m
Loading weights:  79%|███████▊  | 229/291 [00:02<00:00, 84.96it/s, Materializing param=model.layers.25.mlp.experts.gate_up_proj]Loading weights:  79%|███████▊  | 229/291 [00:02<00:00, 84.96it/s, Materializing param=model.layers.25.mlp.experts.down_proj]   Loading weights:  79%|███████▊  | 229/291 [00:02<00:00, 84.96it/s, Materializing param=model.layers.25.mlp.experts.down_proj]Loading weights:  79%|███████▉  | 230/291 [00:02<00:00, 84.96it/s, Materializing param=model.layers.25.mlp.gate.weight]      Loading weights:  79%|███████▉  | 230/291 [00:02<00:00, 84.96it/s, Materializing param=model.layers.25.mlp.gate.weight]Loading weights:  79%|███████▉  | 231/291 [00:02<00:00, 84.96it/s, Materializing param=model.layers.25.input_layernorm.weight]Loading weights:  79%|███████▉  | 231/291 [00:02<00:00, 84.96it/s, Materializing param=model.layers.25.input_layernorm.weight]Loading weights:  80%|███████▉  | 232/291 [00:02<00:00, 84.96it/s, Materializing param=model.layers.25.post_attention_layernorm.weight]Loading weights:  80%|███████▉  | 232/291 [00:02<00:00, 84.96it/s, Materializing param=model.layers.25.post_attention_layernorm.weight]Loading weights:  80%|████████  | 233/291 [00:02<00:00, 84.96it/s, Materializing param=model.layers.25.self_attn.k_proj.weight]        Loading weights:  80%|████████  | 233/291 [00:02<00:00, 84.96it/s, Materializing param=model.layers.25.self_attn.k_proj.weight]Loading weights:  80%|████████  | 234/291 [00:02<00:00, 84.96it/s, Materializing param=model.layers.25.self_attn.o_proj.weight]Loading weights:  80%|████████  | 234/291 [00:02<00:00, 84.96it/s, Materializing param=model.layers.25.self_attn.o_proj.weight]Loading weights:  81%|████████  | 235/291 [00:02<00:00, 84.96it/s, Materializing param=model.layers.25.self_attn.q_proj.weight]Loading weights:  81%|████████  | 235/291 [00:02<00:00, 84.96it/s, Materializing param=model.layers.25.self_attn.q_proj.weight]Loading weights:  81%|████████  | 236/291 [00:02<00:00, 84.96it/s, Materializing param=model.layers.25.self_attn.v_proj.weight]Loading weights:  81%|████████  | 236/291 [00:02<00:00, 84.96it/s, Materializing param=model.layers.25.self_attn.v_proj.weight]Loading weights:  81%|████████▏ | 237/291 [00:02<00:00, 84.96it/s, Materializing param=model.layers.26.mlp.experts.gate_up_proj]Loading weights:  81%|████████▏ | 237/291 [00:02<00:00, 84.96it/s, Materializing param=model.layers.26.mlp.experts.gate_up_proj]Loading weights:  82%|████████▏ | 238/291 [00:02<00:00, 85.32it/s, Materializing param=model.layers.26.mlp.experts.gate_up_proj]Loading weights:  82%|████████▏ | 238/291 [00:02<00:00, 85.32it/s, Materializing param=model.layers.26.mlp.experts.down_proj]   Loading weights:  82%|████████▏ | 238/291 [00:02<00:00, 85.32it/s, Materializing param=model.layers.26.mlp.experts.down_proj]Loading weights:  82%|████████▏ | 239/291 [00:02<00:00, 85.32it/s, Materializing param=model.layers.26.mlp.gate.weight]      Loading weights:  82%|████████▏ | 239/291 [00:02<00:00, 85.32it/s, Materializing param=model.layers.26.mlp.gate.weight]Loading weights:  82%|████████▏ | 240/291 [00:02<00:00, 85.32it/s, Materializing param=model.layers.26.input_layernorm.weight]Loading weights:  82%|████████▏ | 240/291 [00:02<00:00, 85.32it/s, Materializing param=model.layers.26.input_layernorm.weight]Loading weights:  83%|████████▎ | 241/291 [00:02<00:00, 85.32it/s, Materializing param=model.layers.26.post_attention_layernorm.weight]Loading weights:  83%|████████▎ | 241/291 [00:02<00:00, 85.32it/s, Materializing param=model.layers.26.post_attention_layernorm.weight]Loading weights:  83%|████████▎ | 242/291 [00:02<00:00, 85.32it/s, Materializing param=model.layers.26.self_attn.k_proj.weight]        Loading weights:  83%|████████▎ | 242/291 [00:02<00:00, 85.32it/s, Materializing param=model.layers.26.self_attn.k_proj.weight]Loading weights:  84%|████████▎ | 243/291 [00:02<00:00, 85.32it/s, Materializing param=model.layers.26.self_attn.o_proj.weight]Loading weights:  84%|████████▎ | 243/291 [00:02<00:00, 85.32it/s, Materializing param=model.layers.26.self_attn.o_proj.weight]Loading weights:  84%|████████▍ | 244/291 [00:02<00:00, 85.32it/s, Materializing param=model.layers.26.self_attn.q_proj.weight]Loading weights:  84%|████████▍ | 244/291 [00:02<00:00, 85.32it/s, Materializing param=model.layers.26.self_attn.q_proj.weight]Loading weights:  84%|████████▍ | 245/291 [00:02<00:00, 85.32it/s, Materializing param=model.layers.26.self_attn.v_proj.weight]Loading weights:  84%|████████▍ | 245/291 [00:02<00:00, 85.32it/s, Materializing param=model.layers.26.self_attn.v_proj.weight]Loading weights:  85%|████████▍ | 246/291 [00:02<00:00, 85.32it/s, Materializing param=model.layers.27.mlp.experts.gate_up_proj]Loading weights:  85%|████████▍ | 246/291 [00:02<00:00, 85.32it/s, Materializing param=model.layers.27.mlp.experts.gate_up_proj]Loading weights:  85%|████████▍ | 247/291 [00:03<00:00, 85.58it/s, Materializing param=model.layers.27.mlp.experts.gate_up_proj]Loading weights:  85%|████████▍ | 247/291 [00:03<00:00, 85.58it/s, Materializing param=model.layers.27.mlp.experts.down_proj]   Loading weights:  85%|████████▍ | 247/291 [00:03<00:00, 85.58it/s, Materializing param=model.layers.27.mlp.experts.down_proj]Loading weights:  85%|████████▌ | 248/291 [00:03<00:00, 85.58it/s, Materializing param=model.layers.27.mlp.gate.weight]      Loading weights:  85%|████████▌ | 248/291 [00:03<00:00, 85.58it/s, Materializing param=model.layers.27.mlp.gate.weight]Loading weights:  86%|████████▌ | 249/291 [00:03<00:00, 85.58it/s, Materializing param=model.layers.27.input_layernorm.weight]Loading weights:  86%|████████▌ | 249/291 [00:03<00:00, 85.58it/s, Materializing param=model.layers.27.input_layernorm.weight]Loading weights:  86%|████████▌ | 250/291 [00:03<00:00, 85.58it/s, Materializing param=model.layers.27.post_attention_layernorm.weight]Loading weights:  86%|████████▌ | 250/291 [00:03<00:00, 85.58it/s, Materializing param=model.layers.27.post_attention_layernorm.weight]Loading weights:  86%|████████▋ | 251/291 [00:03<00:00, 85.58it/s, Materializing param=model.layers.27.self_attn.k_proj.weight]        Loading weights:  86%|████████▋ | 251/291 [00:03<00:00, 85.58it/s, Materializing param=model.layers.27.self_attn.k_proj.weight]Loading weights:  87%|████████▋ | 252/291 [00:03<00:00, 85.58it/s, Materializing param=model.layers.27.self_attn.o_proj.weight]Loading weights:  87%|████████▋ | 252/291 [00:03<00:00, 85.58it/s, Materializing param=model.layers.27.self_attn.o_proj.weight]Loading weights:  87%|████████▋ | 253/291 [00:03<00:00, 85.58it/s, Materializing param=model.layers.27.self_attn.q_proj.weight]Loading weights:  87%|████████▋ | 253/291 [00:03<00:00, 85.58it/s, Materializing param=model.layers.27.self_attn.q_proj.weight]Loading weights:  87%|████████▋ | 254/291 [00:03<00:00, 85.58it/s, Materializing param=model.layers.27.self_attn.v_proj.weight]Loading weights:  87%|████████▋ | 254/291 [00:03<00:00, 85.58it/s, Materializing param=model.layers.27.self_attn.v_proj.weight]Loading weights:  88%|████████▊ | 255/291 [00:03<00:00, 85.58it/s, Materializing param=model.layers.28.mlp.experts.gate_up_proj]Loading weights:  88%|████████▊ | 255/291 [00:03<00:00, 85.58it/s, Materializing param=model.layers.28.mlp.experts.gate_up_proj]Loading weights:  88%|████████▊ | 256/291 [00:03<00:00, 84.45it/s, Materializing param=model.layers.28.mlp.experts.gate_up_proj]Loading weights:  88%|████████▊ | 256/291 [00:03<00:00, 84.45it/s, Materializing param=model.layers.28.mlp.experts.down_proj]   Loading weights:  88%|████████▊ | 256/291 [00:03<00:00, 84.45it/s, Materializing param=model.layers.28.mlp.experts.down_proj]Loading weights:  88%|████████▊ | 257/291 [00:03<00:00, 84.45it/s, Materializing param=model.layers.28.mlp.gate.weight]      Loading weights:  88%|████████▊ | 257/291 [00:03<00:00, 84.45it/s, Materializing param=model.layers.28.mlp.gate.weight]Loading weights:  89%|████████▊ | 258/291 [00:03<00:00, 84.45it/s, Materializing param=model.layers.28.input_layernorm.weight]Loading weights:  89%|████████▊ | 258/291 [00:03<00:00, 84.45it/s, Materializing param=model.layers.28.input_layernorm.weight]Loading weights:  89%|████████▉ | 259/291 [00:03<00:00, 84.45it/s, Materializing param=model.layers.28.post_attention_layernorm.weight]Loading weights:  89%|████████▉ | 259/291 [00:03<00:00, 84.45it/s, Materializing param=model.layers.28.post_attention_layernorm.weight]Loading weights:  89%|████████▉ | 260/291 [00:03<00:00, 84.45it/s, Materializing param=model.layers.28.self_attn.k_proj.weight]        Loading weights:  89%|████████▉ | 260/291 [00:03<00:00, 84.45it/s, Materializing param=model.layers.28.self_attn.k_proj.weight]Loading weights:  90%|████████▉ | 261/291 [00:03<00:00, 84.45it/s, Materializing param=model.layers.28.self_attn.o_proj.weight]Loading weights:  90%|████████▉ | 261/291 [00:03<00:00, 84.45it/s, Materializing param=model.layers.28.self_attn.o_proj.weight]Loading weights:  90%|█████████ | 262/291 [00:03<00:00, 84.45it/s, Materializing param=model.layers.28.self_attn.q_proj.weight]Loading weights:  90%|█████████ | 262/291 [00:03<00:00, 84.45it/s, Materializing param=model.layers.28.self_attn.q_proj.weight]Loading weights:  90%|█████████ | 263/291 [00:03<00:00, 84.45it/s, Materializing param=model.layers.28.self_attn.v_proj.weight]Loading weights:  90%|█████████ | 263/291 [00:03<00:00, 84.45it/s, Materializing param=model.layers.28.self_attn.v_proj.weight]Loading weights:  91%|█████████ | 264/291 [00:03<00:00, 84.45it/s, Materializing param=model.layers.29.mlp.experts.gate_up_proj]Loading weights:  91%|█████████ | 264/291 [00:03<00:00, 84.45it/s, Materializing param=model.layers.29.mlp.experts.gate_up_proj]Loading weights:  91%|█████████ | 265/291 [00:03<00:00, 84.90it/s, Materializing param=model.layers.29.mlp.experts.gate_up_proj]Loading weights:  91%|█████████ | 265/291 [00:03<00:00, 84.90it/s, Materializing param=model.layers.29.mlp.experts.down_proj]   Loading weights:  91%|█████████ | 265/291 [00:03<00:00, 84.90it/s, Materializing param=model.layers.29.mlp.experts.down_proj]Loading weights:  91%|█████████▏| 266/291 [00:03<00:00, 84.90it/s, Materializing param=model.layers.29.mlp.gate.weight]      Loading weights:  91%|█████████▏| 266/291 [00:03<00:00, 84.90it/s, Materializing param=model.layers.29.mlp.gate.weight]Loading weights:  92%|█████████▏| 267/291 [00:03<00:00, 84.90it/s, Materializing param=model.layers.29.input_layernorm.weight]Loading weights:  92%|█████████▏| 267/291 [00:03<00:00, 84.90it/s, Materializing param=model.layers.29.input_layernorm.weight]Loading weights:  92%|█████████▏| 268/291 [00:03<00:00, 84.90it/s, Materializing param=model.layers.29.post_attention_layernorm.weight]Loading weights:  92%|█████████▏| 268/291 [00:03<00:00, 84.90it/s, Materializing param=model.layers.29.post_attention_layernorm.weight]Loading weights:  92%|█████████▏| 269/291 [00:03<00:00, 84.90it/s, Materializing param=model.layers.29.self_attn.k_proj.weight]        Loading weights:  92%|█████████▏| 269/291 [00:03<00:00, 84.90it/s, Materializing param=model.layers.29.self_attn.k_proj.weight]Loading weights:  93%|█████████▎| 270/291 [00:03<00:00, 84.90it/s, Materializing param=model.layers.29.self_attn.o_proj.weight]Loading weights:  93%|█████████▎| 270/291 [00:03<00:00, 84.90it/s, Materializing param=model.layers.29.self_attn.o_proj.weight]Loading weights:  93%|█████████▎| 271/291 [00:03<00:00, 84.90it/s, Materializing param=model.layers.29.self_attn.q_proj.weight]Loading weights:  93%|█████████▎| 271/291 [00:03<00:00, 84.90it/s, Materializing param=model.layers.29.self_attn.q_proj.weight]Loading weights:  93%|█████████▎| 272/291 [00:03<00:00, 84.90it/s, Materializing param=model.layers.29.self_attn.v_proj.weight]Loading weights:  93%|█████████▎| 272/291 [00:03<00:00, 84.90it/s, Materializing param=model.layers.29.self_attn.v_proj.weight]Loading weights:  94%|█████████▍| 273/291 [00:03<00:00, 84.90it/s, Materializing param=model.layers.30.mlp.experts.gate_up_proj]Loading weights:  94%|█████████▍| 273/291 [00:03<00:00, 84.90it/s, Materializing param=model.layers.30.mlp.experts.gate_up_proj]Loading weights:  94%|█████████▍| 274/291 [00:03<00:00, 85.29it/s, Materializing param=model.layers.30.mlp.experts.gate_up_proj]Loading weights:  94%|█████████▍| 274/291 [00:03<00:00, 85.29it/s, Materializing param=model.layers.30.mlp.experts.down_proj]   Loading weights:  94%|█████████▍| 274/291 [00:03<00:00, 85.29it/s, Materializing param=model.layers.30.mlp.experts.down_proj]Loading weights:  95%|█████████▍| 275/291 [00:03<00:00, 85.29it/s, Materializing param=model.layers.30.mlp.gate.weight]      Loading weights:  95%|█████████▍| 275/291 [00:03<00:00, 85.29it/s, Materializing param=model.layers.30.mlp.gate.weight]Loading weights:  95%|█████████▍| 276/291 [00:03<00:00, 85.29it/s, Materializing param=model.layers.30.input_layernorm.weight]Loading weights:  95%|█████████▍| 276/291 [00:03<00:00, 85.29it/s, Materializing param=model.layers.30.input_layernorm.weight]Loading weights:  95%|█████████▌| 277/291 [00:03<00:00, 85.29it/s, Materializing param=model.layers.30.post_attention_layernorm.weight]Loading weights:  95%|█████████▌| 277/291 [00:03<00:00, 85.29it/s, Materializing param=model.layers.30.post_attention_layernorm.weight]Loading weights:  96%|█████████▌| 278/291 [00:03<00:00, 85.29it/s, Materializing param=model.layers.30.self_attn.k_proj.weight]        Loading weights:  96%|█████████▌| 278/291 [00:03<00:00, 85.29it/s, Materializing param=model.layers.30.self_attn.k_proj.weight]Loading weights:  96%|█████████▌| 279/291 [00:03<00:00, 85.29it/s, Materializing param=model.layers.30.self_attn.o_proj.weight]Loading weights:  96%|█████████▌| 279/291 [00:03<00:00, 85.29it/s, Materializing param=model.layers.30.self_attn.o_proj.weight]Loading weights:  96%|█████████▌| 280/291 [00:03<00:00, 85.29it/s, Materializing param=model.layers.30.self_attn.q_proj.weight]Loading weights:  96%|█████████▌| 280/291 [00:03<00:00, 85.29it/s, Materializing param=model.layers.30.self_attn.q_proj.weight]Loading weights:  97%|█████████▋| 281/291 [00:03<00:00, 85.29it/s, Materializing param=model.layers.30.self_attn.v_proj.weight]Loading weights:  97%|█████████▋| 281/291 [00:03<00:00, 85.29it/s, Materializing param=model.layers.30.self_attn.v_proj.weight]Loading weights:  97%|█████████▋| 282/291 [00:03<00:00, 85.29it/s, Materializing param=model.layers.31.mlp.experts.gate_up_proj]Loading weights:  97%|█████████▋| 282/291 [00:03<00:00, 85.29it/s, Materializing param=model.layers.31.mlp.experts.gate_up_proj]Loading weights:  97%|█████████▋| 283/291 [00:03<00:00, 85.13it/s, Materializing param=model.layers.31.mlp.experts.gate_up_proj]Loading weights:  97%|█████████▋| 283/291 [00:03<00:00, 85.13it/s, Materializing param=model.layers.31.mlp.experts.down_proj]   Loading weights:  97%|█████████▋| 283/291 [00:03<00:00, 85.13it/s, Materializing param=model.layers.31.mlp.experts.down_proj]Loading weights:  98%|█████████▊| 284/291 [00:03<00:00, 85.13it/s, Materializing param=model.layers.31.mlp.gate.weight]      Loading weights:  98%|█████████▊| 284/291 [00:03<00:00, 85.13it/s, Materializing param=model.layers.31.mlp.gate.weight]Loading weights:  98%|█████████▊| 285/291 [00:03<00:00, 85.13it/s, Materializing param=model.layers.31.input_layernorm.weight]Loading weights:  98%|█████████▊| 285/291 [00:03<00:00, 85.13it/s, Materializing param=model.layers.31.input_layernorm.weight]Loading weights:  98%|█████████▊| 286/291 [00:03<00:00, 85.13it/s, Materializing param=model.layers.31.post_attention_layernorm.weight]Loading weights:  98%|█████████▊| 286/291 [00:03<00:00, 85.13it/s, Materializing param=model.layers.31.post_attention_layernorm.weight]Loading weights:  99%|█████████▊| 287/291 [00:03<00:00, 85.13it/s, Materializing param=model.layers.31.self_attn.k_proj.weight]        Loading weights:  99%|█████████▊| 287/291 [00:03<00:00, 85.13it/s, Materializing param=model.layers.31.self_attn.k_proj.weight]Loading weights:  99%|█████████▉| 288/291 [00:03<00:00, 85.13it/s, Materializing param=model.layers.31.self_attn.o_proj.weight]Loading weights:  99%|█████████▉| 288/291 [00:03<00:00, 85.13it/s, Materializing param=model.layers.31.self_attn.o_proj.weight]Loading weights:  99%|█████████▉| 289/291 [00:03<00:00, 85.13it/s, Materializing param=model.layers.31.self_attn.q_proj.weight]Loading weights:  99%|█████████▉| 289/291 [00:03<00:00, 85.13it/s, Materializing param=model.layers.31.self_attn.q_proj.weight]Loading weights: 100%|█████████▉| 290/291 [00:03<00:00, 85.13it/s, Materializing param=model.layers.31.self_attn.v_proj.weight]Loading weights: 100%|█████████▉| 290/291 [00:03<00:00, 85.13it/s, Materializing param=model.layers.31.self_attn.v_proj.weight]Loading weights: 100%|██████████| 291/291 [00:03<00:00, 85.13it/s, Materializing param=model.norm.weight]                      Loading weights: 100%|██████████| 291/291 [00:03<00:00, 85.13it/s, Materializing param=model.norm.weight]Loading weights: 100%|██████████| 291/291 [00:03<00:00, 84.58it/s, Materializing param=model.norm.weight]
[1mMixtralForCausalLM LOAD REPORT[0m from: mistralai/Mixtral-8x7B-Instruct-v0.1
Key                                            | Status     |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      
-----------------------------------------------+------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
model.layers.{0...31}.mlp.experts.gate_up_proj | MISSING    |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      
model.layers.{0...31}.mlp.experts.down_proj    | MISSING    |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      
model.layers.{0...31}.mlp.experts.gate_up_proj | CONVERSION | 

Traceback (most recent call last):
  File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/work/fix-moe-ep/src/transformers/core_model_loading.py", line 832, in log_conversion_errors
    yield
  File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/work/fix-moe-ep/src/transformers/core_model_loading.py", line 724, in convert
    collected_tensors = op.convert(
                        ^^^^^^^^^^^
  File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/env_main/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/work/fix-moe-ep/src/transformers/core_model_loading.py", line 188, in convert
    merged[target_pattern] = torch.stack([k for k in tensors if k != []], dim=self.dim)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: stack expects each tensor to be equal size, but got [14336, 4096] at entry 0 and [] at entry 2
stack expects each tensor to be equal size, but got [14336, 4096] at entry 0 and [] at entry 2
Error: MergeModulelist on tensors destined for model.layers.31.mlp.experts.gate_up_proj. Ckpt contains: 2


model.layers.{0...31}.mlp.experts.down_proj    | CONVERSION | 

Traceback (most recent call last):
  File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/work/fix-moe-ep/src/transformers/core_model_loading.py", line 832, in log_conversion_errors
    yield
  File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/work/fix-moe-ep/src/transformers/core_model_loading.py", line 724, in convert
    collected_tensors = op.convert(
                        ^^^^^^^^^^^
  File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/env_main/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/work/fix-moe-ep/src/transformers/core_model_loading.py", line 188, in convert
    merged[target_pattern] = torch.stack([k for k in tensors if k != []], dim=self.dim)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: stack expects each tensor to be equal size, but got [4096, 14336] at entry 0 and [] at entry 2
stack expects each tensor to be equal size, but got [4096, 14336] at entry 0 and [] at entry 2
Error: MergeModulelist on tensors destined for model.layers.31.mlp.experts.down_proj. Ckpt contains: 1

   

[3mNotes:
- MISSING[3m	:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.
- CONVERSION[3m	:originate from the conversion scheme[0m
[rank0]: Traceback (most recent call last):
[rank0]:   File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/work/fix-moe-ep/tmp_gen.py", line 32, in <module>
[rank0]:     main()
[rank0]:   File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/env_main/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
[rank0]:     return f(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^
[rank0]:   File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/work/fix-moe-ep/tmp_gen.py", line 19, in main
[rank0]:     model = AutoModelForCausalLM.from_pretrained(model_id, dtype=torch.bfloat16, tp_plan="auto")
[rank0]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/work/fix-moe-ep/src/transformers/models/auto/auto_factory.py", line 372, in from_pretrained
[rank0]:     return model_class.from_pretrained(
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/work/fix-moe-ep/src/transformers/modeling_utils.py", line 4077, in from_pretrained
[rank0]:     loading_info = cls._finalize_model_loading(model, load_config, loading_info)
[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/work/fix-moe-ep/src/transformers/modeling_utils.py", line 4238, in _finalize_model_loading
[rank0]:     log_state_dict_report(
[rank0]:   File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/work/fix-moe-ep/src/transformers/utils/loading_report.py", line 273, in log_state_dict_report
[rank0]:     raise RuntimeError(
[rank0]: RuntimeError: We encountered some issues during automatic conversion of the weights. For details look at the `CONVERSION` entries of the above report!
[1mMixtralForCausalLM LOAD REPORT[0m from: mistralai/Mixtral-8x7B-Instruct-v0.1
Key                                            | Status     |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      
-----------------------------------------------+------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
model.layers.{0...31}.mlp.experts.gate_up_proj | MISSING    |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      
model.layers.{0...31}.mlp.experts.down_proj    | MISSING    |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      
model.layers.{0...31}.mlp.experts.gate_up_proj | CONVERSION | 

Traceback (most recent call last):
  File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/work/fix-moe-ep/src/transformers/core_model_loading.py", line 832, in log_conversion_errors
    yield
  File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/work/fix-moe-ep/src/transformers/core_model_loading.py", line 724, in convert
    collected_tensors = op.convert(
                        ^^^^^^^^^^^
  File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/env_main/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/work/fix-moe-ep/src/transformers/core_model_loading.py", line 188, in convert
    merged[target_pattern] = torch.stack([k for k in tensors if k != []], dim=self.dim)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: stack expects each tensor to be equal size, but got [] at entry 0 and [14336, 4096] at entry 6
stack expects each tensor to be equal size, but got [] at entry 0 and [14336, 4096] at entry 6
Error: MergeModulelist on tensors destined for model.layers.31.mlp.experts.gate_up_proj. Ckpt contains: 2


model.layers.{0...31}.mlp.experts.down_proj    | CONVERSION | 

Traceback (most recent call last):
  File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/work/fix-moe-ep/src/transformers/core_model_loading.py", line 832, in log_conversion_errors
    yield
  File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/work/fix-moe-ep/src/transformers/core_model_loading.py", line 724, in convert
    collected_tensors = op.convert(
                        ^^^^^^^^^^^
  File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/env_main/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/work/fix-moe-ep/src/transformers/core_model_loading.py", line 188, in convert
    merged[target_pattern] = torch.stack([k for k in tensors if k != []], dim=self.dim)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: stack expects each tensor to be equal size, but got [] at entry 0 and [4096, 14336] at entry 6
stack expects each tensor to be equal size, but got [] at entry 0 and [4096, 14336] at entry 6
Error: MergeModulelist on tensors destined for model.layers.31.mlp.experts.down_proj. Ckpt contains: 1

   

[3mNotes:
- MISSING[3m	:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.
- CONVERSION[3m	:originate from the conversion scheme[0m
[1mMixtralForCausalLM LOAD REPORT[0m from: mistralai/Mixtral-8x7B-Instruct-v0.1
Key                                            | Status     |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      
-----------------------------------------------+------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
model.layers.{0...31}.mlp.experts.down_proj    | MISSING    |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      
model.layers.{0...31}.mlp.experts.gate_up_proj | MISSING    |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      
model.layers.{0...31}.mlp.experts.gate_up_proj | CONVERSION | 

Traceback (most recent call last):
  File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/work/fix-moe-ep/src/transformers/core_model_loading.py", line 832, in log_conversion_errors
    yield
  File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/work/fix-moe-ep/src/transformers/core_model_loading.py", line 724, in convert
    collected_tensors = op.convert(
                        ^^^^^^^^^^^
  File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/env_main/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/work/fix-moe-ep/src/transformers/core_model_loading.py", line 188, in convert
    merged[target_pattern] = torch.stack([k for k in tensors if k != []], dim=self.dim)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: stack expects each tensor to be equal size, but got [] at entry 0 and [14336, 4096] at entry 4
stack expects each tensor to be equal size, but got [] at entry 0 and [14336, 4096] at entry 4
Error: MergeModulelist on tensors destined for model.layers.31.mlp.experts.gate_up_proj. Ckpt contains: 2


model.layers.{0...31}.mlp.experts.down_proj    | CONVERSION | 

Traceback (most recent call last):
  File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/work/fix-moe-ep/src/transformers/core_model_loading.py", line 832, in log_conversion_errors
    yield
  File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/work/fix-moe-ep/src/transformers/core_model_loading.py", line 724, in convert
    collected_tensors = op.convert(
                        ^^^^^^^^^^^
  File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/env_main/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/work/fix-moe-ep/src/transformers/core_model_loading.py", line 188, in convert
    merged[target_pattern] = torch.stack([k for k in tensors if k != []], dim=self.dim)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: stack expects each tensor to be equal size, but got [] at entry 0 and [4096, 14336] at entry 4
stack expects each tensor to be equal size, but got [] at entry 0 and [4096, 14336] at entry 4
Error: MergeModulelist on tensors destined for model.layers.31.mlp.experts.down_proj. Ckpt contains: 1

   

[3mNotes:
- MISSING[3m	:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.
- CONVERSION[3m	:originate from the conversion scheme[0m
[rank0]:[W205 10:56:10.912654950 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
W0205 10:56:15.262000 2296508 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 2296634 closing signal SIGTERM
W0205 10:56:15.274000 2296508 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 2296636 closing signal SIGTERM
W0205 10:56:15.274000 2296508 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 2296637 closing signal SIGTERM
E0205 10:56:16.617000 2296508 torch/distributed/elastic/multiprocessing/api.py:882] failed (exitcode: 1) local_rank: 1 (pid: 2296635) of binary: /fsx/ferdinandmom/ferdinand-hf/transformers_pr/env_main/bin/python3
E0205 10:56:16.638000 2296508 torch/distributed/elastic/multiprocessing/errors/error_handler.py:141] no error file defined for parent, to copy child error file (/tmp/torchelastic_tpa3nl_q/none_k2dyiyzj/attempt_0/1/error.json)
Traceback (most recent call last):
  File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/env_main/bin/torchrun", line 10, in <module>
    sys.exit(main())
             ^^^^^^
  File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/env_main/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/env_main/lib/python3.12/site-packages/torch/distributed/run.py", line 936, in main
    run(args)
  File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/env_main/lib/python3.12/site-packages/torch/distributed/run.py", line 927, in run
    elastic_launch(
  File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/env_main/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 156, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/env_main/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 293, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
tmp_gen.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2026-02-05_10:56:09
  host      : ip-26-0-164-207.ec2.internal
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 2296635)
  error_file: /tmp/torchelastic_tpa3nl_q/none_k2dyiyzj/attempt_0/1/error.json
  traceback : Traceback (most recent call last):
    File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/env_main/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
      return f(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^
    File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/work/fix-moe-ep/tmp_gen.py", line 19, in main
      model = AutoModelForCausalLM.from_pretrained(model_id, dtype=torch.bfloat16, tp_plan="auto")
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/work/fix-moe-ep/src/transformers/models/auto/auto_factory.py", line 372, in from_pretrained
      return model_class.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/work/fix-moe-ep/src/transformers/modeling_utils.py", line 4077, in from_pretrained
      loading_info = cls._finalize_model_loading(model, load_config, loading_info)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/work/fix-moe-ep/src/transformers/modeling_utils.py", line 4238, in _finalize_model_loading
      log_state_dict_report(
    File "/fsx/ferdinandmom/ferdinand-hf/transformers_pr/work/fix-moe-ep/src/transformers/utils/loading_report.py", line 273, in log_state_dict_report
      raise RuntimeError(
  RuntimeError: We encountered some issues during automatic conversion of the weights. For details look at the `CONVERSION` entries of the above report!
  
============================================================
