name: model jobs
on:
  workflow_call:
    inputs:
      folder_slices:
        required: true
        type: string
      machine_type:
        required: true
        type: string
      slice_id:
        required: true
        type: number
      runner_map:
        required: false
        type: string
      docker:
        required: true
        type: string
      commit_sha:
        required: false
        type: string
      report_name_prefix:
        required: false
        default: run_models_gpu
        type: string
      commit:
        required: false
        type: string
env:
  HF_HOME: /mnt/cache
  TRANSFORMERS_IS_CI: yes
  OMP_NUM_THREADS: 8
  MKL_NUM_THREADS: 8
  RUN_SLOW: yes
  HF_HUB_READ_TOKEN: ${{ secrets.HF_HUB_READ_TOKEN }}
  SIGOPT_API_TOKEN: ${{ secrets.SIGOPT_API_TOKEN }}
  TF_FORCE_GPU_ALLOW_GROWTH: true
  CUDA_VISIBLE_DEVICES: 0,1
jobs:
  run_models_gpu:
    name: ' '
    strategy:
      max-parallel: 8
      fail-fast: false
      matrix:
        folders: ${{ fromJson(inputs.folder_slices)[inputs.slice_id] }}
    runs-on: ubuntu-latest
    container:
      image: ${{ inputs.docker }}
      options: --gpus all --shm-size "16gb" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/
    steps:
    - name: Echo input and matrix info
      shell: bash
      run: 'echo "${{ inputs.folder_slices }}"

        echo "${{ matrix.folders }}"

        echo "${{ toJson(fromJson(inputs.folder_slices)[inputs.slice_id]) }}"

        '
    - name: Echo folder ${{ matrix.folders }}
      shell: bash
      run: 'echo "${{ matrix.folders }}"

        matrix_folders=${{ matrix.folders }}

        matrix_folders=${matrix_folders/''models/''/''models_''}

        echo "$matrix_folders"

        echo "matrix_folders=$matrix_folders" >> $GITHUB_ENV

        '
    - name: Update clone
      working-directory: /transformers
      run: git fetch && git checkout ${{ inputs.commit_sha || github.sha }}
    - name: Reinstall transformers in edit mode (remove the one installed during docker
        image build)
      working-directory: /transformers
      run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .
    - name: Update / Install some packages (for Past CI)
      if: ${{ contains(inputs.docker, '-past-') }}
      working-directory: /transformers
      run: 'python3 -m pip install -U datasets

        '
    - name: Update / Install some packages (for Past CI)
      if: ${{ contains(inputs.docker, '-past-') && contains(inputs.docker, '-pytorch-')
        }}
      working-directory: /transformers
      run: 'python3 -m pip install --no-cache-dir git+https://github.com/huggingface/accelerate@main#egg=accelerate

        '
    - name: NVIDIA-SMI
      run: 'nvidia-smi

        '
    - name: Environment
      working-directory: /transformers
      run: 'python3 utils/print_env.py

        '
    - name: Show installed libraries and their versions
      working-directory: /transformers
      run: pip freeze
    - name: Set `machine_type` for report and artifact names
      working-directory: /transformers
      shell: bash
      run: "echo \"${{ inputs.machine_type }}\"\n\nif [ \"${{ inputs.machine_type\
        \ }}\" = \"aws-g5-4xlarge-cache\" ]; then\n  machine_type=single-gpu\nelif\
        \ [ \"${{ inputs.machine_type }}\" = \"aws-g5-12xlarge-cache\" ]; then\n \
        \ machine_type=multi-gpu\nelse\n  machine_type=${{ inputs.machine_type }}\n\
        fi\n\necho \"$machine_type\"\necho \"machine_type=$machine_type\" >> $GITHUB_ENV\n"
    - name: Uninstall FA2
      working-directory: /transformers
      run: 'python3 -m pip uninstall -y flash-attn

        python3 -m pip uninstall -y accelerate && python3 -m pip install --no-cache-dir
        git+https://github.com/huggingface/accelerate@7c25f696b87e0f82cdb915cdba3945cce3ded0bc#egg=accelerate

        '
    - name: Run all tests on GPU
      working-directory: /transformers
      run: "free -g\ngit checkout 2c55c7fc\n# Start memory monitoring in background\n\
        while true; do\n  echo \"$(date): Memory usage:\"\n  free -g\n  sleep 1\n\
        done &\n\n# Store the background job PID\nMONITOR_PID=$!\n\n# Run the test\n\
        python3 -m pytest -v tests/models/qwen2_moe/test_modeling_qwen2_moe.py::Qwen2MoeIntegrationTest\n\
        \n# Kill the background monitoring\nkill $MONITOR_PID\n"
