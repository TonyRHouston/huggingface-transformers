[rules.TRF001]
description = "PreTrainedModel subclasses that define __init__ must initialize a PreTrainedModel parent."
default_enabled = true

[rules.TRF001.explanation]
what_it_does = "Checks that PreTrainedModel subclasses call a parent __init__ when they define __init__."
why_bad = "Skipping parent initialization can leave model internals uninitialized and cause subtle runtime bugs."
bad_example = '''
class FooPreTrainedModel(PreTrainedModel):
    def __init__(self, config):
        self.config = config
'''
good_example = '''
class FooPreTrainedModel(PreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.config = config
'''

[rules.TRF002]
description = "When modular_<name>.py exists, modeling_<name>.py must keep the generated banner markers."
default_enabled = true
allowlist_models = ["colpali", "dpt", "efficientloftr", "grounding_dino", "mask2former", "owlv2", "sam3", "segformer", "yolos"]

[rules.TRF002.explanation]
what_it_does = "Checks that generated modeling files keep the canonical modular-generation banner."
why_bad = "Editing generated files directly creates drift from modular sources and future regeneration will overwrite changes."
bad_example = '''
# Copyright ...
# Licensed under the Apache License...
# (no generated-file banner present)
'''
good_example = '''
# This file was automatically generated from src/transformers/models/foo/modular_foo.py.
# Do NOT edit this file manually as any edits will be overwritten ...
'''

[rules.TRF003]
description = "Class-level config_class on <Model>PreTrainedModel should match <Model>Config naming."
default_enabled = true
allowlist_models = ["qwen3_omni_moe"]

[rules.TRF003.explanation]
what_it_does = "Checks naming consistency between <Model>PreTrainedModel and config_class."
why_bad = "Mismatched config_class can break loading, auto classes, and developer expectations."
bad_example = '''
class FooPreTrainedModel(PreTrainedModel):
    config_class = BarConfig
'''
good_example = '''
class FooPreTrainedModel(PreTrainedModel):
    config_class = FooConfig
'''

[rules.TRF004]
description = "base_model_prefix should be a non-empty canonical string when defined on PreTrainedModel classes."
default_enabled = true
allowlist_models = ["lighton_ocr"]

[rules.TRF004.explanation]
what_it_does = "Checks that base_model_prefix, when set, is a non-empty, whitespace-free string literal."
why_bad = "Invalid prefixes can break weight loading key mapping and base model access patterns."
bad_example = '''
class FooPreTrainedModel(PreTrainedModel):
    base_model_prefix = ""
'''
good_example = '''
class FooPreTrainedModel(PreTrainedModel):
    base_model_prefix = "model"
'''

[rules.TRF005]
description = "forward(return_dict=...) should use return_dict in control flow or output construction."
default_enabled = true
allowlist_models = ["chinese_clip", "ovis2", "pixtral"]

[rules.TRF005.explanation]
what_it_does = "Checks that forward methods with a return_dict argument actually reference it."
why_bad = "Ignoring return_dict creates inconsistent return contracts and can break callers."
bad_example = '''
def forward(self, x, return_dict=None):
    return FooOutput(last_hidden_state=x)
'''
good_example = '''
def forward(self, x, return_dict=None):
    if not return_dict:
        return (x,)
    return FooOutput(last_hidden_state=x)
'''

[rules.TRF006]
description = "supports_gradient_checkpointing=True should be paired with gradient checkpointing implementation hooks."
default_enabled = true

[rules.TRF006.explanation]
what_it_does = "Checks that models advertising gradient checkpointing support expose expected implementation hooks."
why_bad = "Declaring support without hooks can mislead users and fail at runtime when checkpointing is enabled."
bad_example = '''
class FooPreTrainedModel(PreTrainedModel):
    supports_gradient_checkpointing = True
'''
good_example = '''
class FooPreTrainedModel(PreTrainedModel):
    supports_gradient_checkpointing = True
    def _set_gradient_checkpointing(self, module, value=False):
        module.gradient_checkpointing = value
'''

[rules.TRF007]
description = "Custom tie_weights should route through output-embedding helpers or delegate to super().tie_weights()."
default_enabled = true
allowlist_models = ["data2vec", "hubert", "sew", "sew_d", "unispeech", "unispeech_sat", "wav2vec2", "wav2vec2_conformer", "wavlm"]

[rules.TRF007.explanation]
what_it_does = "Checks tie_weights implementations for standard helper usage or parent delegation."
why_bad = "Ad-hoc tying can miss edge cases and diverge from shared loading/saving behavior."
bad_example = '''
def tie_weights(self):
    self.lm_head.weight = self.emb.weight
'''
good_example = '''
def tie_weights(self):
    super().tie_weights()
    # or use get_output_embeddings/set_output_embeddings helpers
'''

[rules.TRF008]
description = "_no_split_modules, when defined, should be a list/tuple of non-empty strings."
default_enabled = true
allowlist_models = ["d_fine", "deformable_detr", "glm46v", "lw_detr", "pp_doclayout_v3", "rt_detr", "rt_detr_v2", "voxtral", "voxtral_realtime"]

[rules.TRF008.explanation]
what_it_does = "Checks the shape of _no_split_modules when present."
why_bad = "Malformed values can break device-map partitioning and sharding behavior."
bad_example = '''
_no_split_modules = [SomeLayerClass, ""]
'''
good_example = '''
_no_split_modules = ["FooDecoderLayer", "FooAttention"]
'''

[rules.TRF009]
description = "Classes directly mixing GenerationMixin should provide generation input preparation hooks."
default_enabled = true

[rules.TRF009.explanation]
what_it_does = "Checks directly mixed-in GenerationMixin classes for explicit generation hooks when no other parent class exists."
why_bad = "A standalone GenerationMixin class without hooks may expose incomplete generation behavior."
bad_example = '''
class FooLM(GenerationMixin):
    pass
'''
good_example = '''
class FooLM(GenerationMixin):
    def prepare_inputs_for_generation(self, input_ids, **kwargs):
        return {"input_ids": input_ids}
'''

[rules.TRF010]
description = "forward with cache arguments should reference cache control/state variables consistently."
default_enabled = true
allowlist_models = ["chinese_clip", "evolla", "idefics2", "llama4"]

[rules.TRF010.explanation]
what_it_does = "Checks forward signatures that expose cache arguments for usage of those arguments in method body."
why_bad = "Unused cache arguments can indicate incomplete caching support and inconsistent API behavior."
bad_example = '''
def forward(self, x, past_key_values=None, use_cache=False):
    return x
'''
good_example = '''
def forward(self, x, past_key_values=None, use_cache=False):
    if use_cache:
        ...
    return x
'''

[rules.TRF011]
description = "self.post_init() in __init__ should remain at the end of initialization for PreTrainedModel classes."
default_enabled = true
allowlist_models = ["distilbert", "lxmert", "mt5", "pix2struct", "pop2piano", "switch_transformers", "t5"]

[rules.TRF011.explanation]
what_it_does = "Checks for self attribute assignments after self.post_init() in __init__."
why_bad = "Mutating model structure after post_init can bypass intended initialization/finalization logic."
bad_example = '''
def __init__(self, config):
    ...
    self.post_init()
    self.proj = nn.Linear(...)
'''
good_example = '''
def __init__(self, config):
    ...
    self.proj = nn.Linear(...)
    self.post_init()
'''

[rules.TRF012]
description = "Doc decorators on PreTrainedModel classes should avoid empty add_start_docstrings usage."
default_enabled = true

[rules.TRF012.explanation]
what_it_does = "Checks add_start_docstrings usage on model classes for non-empty docstring arguments."
why_bad = "Empty decorator usage produces unclear docs and weakens generated API documentation quality."
bad_example = '''
@add_start_docstrings("")
class FooModel(FooPreTrainedModel):
    ...
'''
good_example = '''
@add_start_docstrings("The Foo model.")
class FooModel(FooPreTrainedModel):
    ...
'''

[rules.TRF013]
description = "modeling_<name>.py should avoid importing implementation code from another model package."
default_enabled = false
allowlist_models = []

[rules.TRF013.explanation]
what_it_does = "Checks modeling files for cross-model imports such as transformers.models.other_model.* or from ..other_model.* imports."
why_bad = "Cross-model implementation imports violate the single-file policy and make model behavior harder to inspect and maintain."
bad_example = '''
from transformers.models.llama.modeling_llama import LlamaAttention
'''
good_example = '''
# Keep implementation local in this modeling file.
# If code is reused, copy it with an appropriate # Copied from ... statement.
'''
