#!/usr/bin/env python
# Copyright 2025 The HuggingFace Inc. team.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
Utility that ensures that modeling (and modular) files respect some important conventions we have in Transformers.
"""

import argparse
import ast
import subprocess
import sys
from contextlib import nullcontext
from dataclasses import dataclass
from pathlib import Path

from rich import print
from rich.console import Console


try:
    import tomllib  # Python >= 3.11
except ModuleNotFoundError:
    import tomli as tomllib  # Python 3.10 fallback


MODELS_ROOT = Path("src/transformers/models")
MODELING_PATTERNS = ("modeling_*.py", "modular_*.py")
TRF001 = "TRF001"
TRF002 = "TRF002"
TRF003 = "TRF003"
TRF004 = "TRF004"
TRF005 = "TRF005"
TRF006 = "TRF006"
TRF007 = "TRF007"
TRF008 = "TRF008"
TRF009 = "TRF009"
TRF010 = "TRF010"
TRF011 = "TRF011"
TRF012 = "TRF012"
RULE_SPECS_PATH = Path(__file__).with_name("check_modeling_structure_rules.toml")


def _load_rule_specs() -> dict[str, dict]:
    data = tomllib.loads(RULE_SPECS_PATH.read_text(encoding="utf-8"))
    rules = data.get("rules")
    if not isinstance(rules, dict):
        raise ValueError(f"Invalid rule spec file: missing [rules] table in {RULE_SPECS_PATH}")

    required_explanation_keys = {"what_it_does", "why_bad", "bad_example", "good_example"}
    specs: dict[str, dict] = {}
    for rule_id, spec in rules.items():
        if not isinstance(spec, dict):
            raise ValueError(f"Invalid rule spec for {rule_id}: expected table")
        description = spec.get("description")
        default_enabled = spec.get("default_enabled")
        explanation = spec.get("explanation")
        if not isinstance(description, str) or not description.strip():
            raise ValueError(f"Invalid rule spec for {rule_id}: missing non-empty description")
        if not isinstance(default_enabled, bool):
            raise ValueError(f"Invalid rule spec for {rule_id}: default_enabled must be bool")
        if not isinstance(explanation, dict) or not required_explanation_keys.issubset(explanation):
            raise ValueError(f"Invalid rule spec for {rule_id}: incomplete explanation block")
        if any(not isinstance(explanation[key], str) for key in required_explanation_keys):
            raise ValueError(f"Invalid rule spec for {rule_id}: explanation values must be strings")

        allowlist_models = spec.get("allowlist_models", [])
        if not isinstance(allowlist_models, list) or any(not isinstance(item, str) for item in allowlist_models):
            raise ValueError(f"Invalid rule spec for {rule_id}: allowlist_models must be list[str]")

        specs[rule_id] = {
            "description": description,
            "default_enabled": default_enabled,
            "explanation": explanation,
            "allowlist_models": set(allowlist_models),
        }

    return specs


TRF_RULE_SPECS = _load_rule_specs()
TRF_RULES = {rule_id: spec["description"] for rule_id, spec in TRF_RULE_SPECS.items()}
DEFAULT_ENABLED_TRF_RULES = {rule_id for rule_id, spec in TRF_RULE_SPECS.items() if spec["default_enabled"]}
BANNER_MARKERS = (
    "This file was automatically generated from",
    "Do NOT edit this file manually",
)
# Model-directory baseline allowlists for existing legacy exceptions.
# Keep each set as small as possible and remove entries when those models are migrated.
TRF_MODEL_DIR_ALLOWLISTS = {
    rule_id: spec["allowlist_models"] for rule_id, spec in TRF_RULE_SPECS.items() if spec["allowlist_models"]
}
CONSOLE = Console(stderr=True)


@dataclass(frozen=True)
class Violation:
    file_path: Path
    line_number: int
    message: str
    rule_id: str | None = None


def _validate_rule_ids(rule_ids: set[str]) -> set[str]:
    unknown = sorted(rule_id for rule_id in rule_ids if rule_id not in TRF_RULES)
    if unknown:
        raise ValueError(
            f"Unknown rule id(s): {', '.join(unknown)}. Valid rules: {', '.join(sorted(TRF_RULES))}"
        )
    return rule_ids


def iter_modeling_files(paths: set[Path] | None = None):
    if paths is None:
        for pattern in MODELING_PATTERNS:
            yield from MODELS_ROOT.rglob(pattern)
        return

    for path in sorted(paths):
        if path.exists():
            yield path


def colored_error_message(file_path: str, line_number: int, message: str) -> str:
    return f"[bold red]{file_path}[/bold red]:[bold yellow]L{line_number}[/bold yellow]: {message}"


def full_name(node: ast.AST):
    """
    Return full dotted name from an Attribute or Name node.
    """
    if isinstance(node, ast.Name):
        return node.id
    elif isinstance(node, ast.Attribute):
        return full_name(node.value) + "." + node.attr
    else:
        raise ValueError("Not a Name or Attribute node")


def _simple_name(name: str) -> str:
    return name.split(".")[-1]


def _model_dir_name(file_path: Path) -> str | None:
    try:
        relative = file_path.resolve().relative_to(MODELS_ROOT.resolve())
    except ValueError:
        try:
            relative = file_path.relative_to(MODELS_ROOT)
        except ValueError:
            return None
    if len(relative.parts) < 2:
        return None
    return relative.parts[0]


def _is_rule_allowlisted_for_file(rule_id: str, file_path: Path) -> bool:
    model_name = _model_dir_name(file_path)
    if model_name is None:
        return False
    return model_name in TRF_MODEL_DIR_ALLOWLISTS.get(rule_id, set())


def _has_rule_suppression(lines: list[str], rule_id: str, line_number: int) -> bool:
    if line_number <= 0:
        return False

    token = f"trf-ignore: {rule_id}".lower()
    candidate_indexes = (line_number - 1, line_number - 2)
    for idx in candidate_indexes:
        if 0 <= idx < len(lines) and token in lines[idx].lower():
            return True
    return False


def check_init_weights(node: ast.AST, violations: list[Violation], file_path: Path) -> list[Violation]:
    """
    Check that `_init_weights` correctly use `init.(...)` patterns to init the weights in-place. This is very important,
    as we rely on the internal flag set on the parameters themselves to check if they need to be re-init or not.
    """
    if isinstance(node, ast.FunctionDef) and node.name == "_init_weights":
        args = node.args.args
        if len(args) < 2 or getattr(args[0], "arg", None) != "self" or getattr(args[1], "arg", None) != "module":
            return violations

        for sub_node in ast.walk(node):
            if isinstance(sub_node, ast.Call) and isinstance(sub_node.func, ast.Attribute):
                is_inplace_ops = sub_node.func.attr.endswith("_")
                # We allow in-place ops on tensors that are not part of the module itself (see e.g. modeling_qwen3_next.py L997)
                is_on_module_weight = isinstance(
                    sub_node.func.value, (ast.Name, ast.Attribute)
                ) and "module." in full_name(sub_node.func.value)
                if is_inplace_ops and is_on_module_weight:
                    error_msg = (
                        "`_init_weights(self, module)` uses an in-place operation on a module's weight. Please use the "
                        "`init` functions primitives instead, usually imported as `from ... import initialization as init`"
                    )
                    violations.append(Violation(file_path=file_path, line_number=sub_node.lineno, message=error_msg))

    return violations


def is_self_method_call(node: ast.AST, method: str) -> bool:
    """Check if `node` is a method call on `self`, such as `self.method(...)`"""
    return (
        isinstance(node, ast.Call)
        and isinstance(node.func, ast.Attribute)
        and isinstance(node.func.value, ast.Name)
        and node.func.value.id == "self"
        and node.func.attr == method
    )


def is_super_method_call(node: ast.AST, method: str) -> bool:
    """Check if `node` is a call to `super().method(...)`"""
    return (
        isinstance(node, ast.Call)
        and isinstance(node.func, ast.Attribute)
        and isinstance(node.func.value, ast.Call)
        and isinstance(node.func.value.func, ast.Name)
        and node.func.value.func.id == "super"
        and node.func.attr == method
    )


def check_post_init(node: ast.AST, violations: list[Violation], file_path: Path) -> list[Violation]:
    """
    Check that `self.post_init()` is correctly called at the end of `__init__` for all `PreTrainedModel`s. This is
    very important as we need to do some processing there.
    """
    # Check if it's a PreTrainedModel class definition
    if isinstance(node, ast.ClassDef) and any(full_name(parent).endswith("PreTrainedModel") for parent in node.bases):
        for sub_node in node.body:
            # Check that we are in __init__
            if isinstance(sub_node, ast.FunctionDef) and sub_node.name == "__init__":
                for statement in ast.walk(sub_node):
                    # This means it's correctly called verbatim
                    if is_self_method_call(statement, method="post_init"):
                        break
                    # This means `super().__init__` is called in a modular, so it is already called in the parent
                    elif "modular_" in str(file_path) and is_super_method_call(statement, method="__init__"):
                        break
                # If we did not break, `post_init` was never called
                else:
                    error_msg = f"`__init__` of {node.name} does not call `self.post_init`"
                    violations.append(Violation(file_path=file_path, line_number=sub_node.lineno, message=error_msg))
                break

    return violations


def _is_super_init_call(node: ast.AST) -> bool:
    return is_super_method_call(node, method="__init__")


def _is_explicit_pretrained_parent_init_call(node: ast.AST) -> bool:
    if not (isinstance(node, ast.Call) and isinstance(node.func, ast.Attribute) and node.func.attr == "__init__"):
        return False

    if not node.args:
        return False

    first_arg = node.args[0]
    if not (isinstance(first_arg, ast.Name) and first_arg.id == "self"):
        return False

    try:
        callee_name = full_name(node.func.value)
    except ValueError:
        return False

    return _simple_name(callee_name).endswith("PreTrainedModel")


def _collect_class_bases(tree: ast.Module) -> dict[str, list[str]]:
    class_to_bases: dict[str, list[str]] = {}
    for node in tree.body:
        if not isinstance(node, ast.ClassDef):
            continue
        base_names = []
        for base in node.bases:
            try:
                base_names.append(full_name(base))
            except ValueError:
                continue
        class_to_bases[node.name] = base_names
    return class_to_bases


def _inherits_pretrained_model(
    class_name: str, class_to_bases: dict[str, list[str]], visiting: set[str] | None = None
) -> bool:
    if visiting is None:
        visiting = set()
    if class_name in visiting:
        return False
    visiting.add(class_name)

    for base_name in class_to_bases.get(class_name, []):
        simple_base_name = _simple_name(base_name)
        if simple_base_name.endswith("PreTrainedModel"):
            return True
        if simple_base_name in class_to_bases and _inherits_pretrained_model(
            simple_base_name, class_to_bases, visiting
        ):
            return True
    return False


def check_pretrainedmodel_parent_init(
    tree: ast.Module, file_path: Path, source_lines: list[str], violations: list[Violation]
) -> list[Violation]:
    class_to_bases = _collect_class_bases(tree)

    for node in tree.body:
        if not isinstance(node, ast.ClassDef):
            continue
        if not _inherits_pretrained_model(node.name, class_to_bases):
            continue

        init_method = next(
            (
                class_item
                for class_item in node.body
                if isinstance(class_item, ast.FunctionDef) and class_item.name == "__init__"
            ),
            None,
        )
        if init_method is None:
            continue

        if _has_rule_suppression(source_lines, TRF001, node.lineno) or _has_rule_suppression(
            source_lines, TRF001, init_method.lineno
        ):
            continue

        if any(
            _is_super_init_call(sub_node) or _is_explicit_pretrained_parent_init_call(sub_node)
            for sub_node in ast.walk(init_method)
        ):
            continue

        error_msg = (
            f"{TRF001}: __init__ of {node.name} must initialize a PreTrainedModel parent "
            "(super().__init__(...) or explicit <Base>.__init__(self, ...))."
        )
        violations.append(
            Violation(file_path=file_path, line_number=init_method.lineno, message=error_msg, rule_id=TRF001)
        )

    return violations


def _get_class_assignments(class_node: ast.ClassDef) -> dict[str, ast.AST]:
    assignments: dict[str, ast.AST] = {}
    for item in class_node.body:
        if isinstance(item, ast.Assign) and len(item.targets) == 1 and isinstance(item.targets[0], ast.Name):
            assignments[item.targets[0].id] = item.value
        elif isinstance(item, ast.AnnAssign) and isinstance(item.target, ast.Name) and item.value is not None:
            assignments[item.target.id] = item.value
    return assignments


def _class_methods(class_node: ast.ClassDef) -> dict[str, ast.FunctionDef]:
    return {item.name: item for item in class_node.body if isinstance(item, ast.FunctionDef)}


def check_config_class_consistency(
    tree: ast.Module, file_path: Path, source_lines: list[str], violations: list[Violation]
) -> list[Violation]:
    class_to_bases = _collect_class_bases(tree)
    for node in tree.body:
        if not isinstance(node, ast.ClassDef):
            continue
        if not node.name.endswith("PreTrainedModel"):
            continue
        if not _inherits_pretrained_model(node.name, class_to_bases):
            continue
        if _has_rule_suppression(source_lines, TRF003, node.lineno):
            continue

        assignments = _get_class_assignments(node)
        config_value = assignments.get("config_class")
        if config_value is None:
            continue
        if not isinstance(config_value, (ast.Name, ast.Attribute)):
            continue

        config_name = _simple_name(full_name(config_value))
        expected = f"{node.name.removesuffix('PreTrainedModel')}Config"
        if config_name != expected:
            violations.append(
                Violation(
                    file_path=file_path,
                    line_number=getattr(config_value, "lineno", node.lineno),
                    rule_id=TRF003,
                    message=(
                        f"{TRF003}: {node.name}.config_class is {config_name}, expected {expected}."
                    ),
                )
            )

    return violations


def check_base_model_prefix(
    tree: ast.Module, file_path: Path, source_lines: list[str], violations: list[Violation]
) -> list[Violation]:
    class_to_bases = _collect_class_bases(tree)
    for node in tree.body:
        if not isinstance(node, ast.ClassDef):
            continue
        if not _inherits_pretrained_model(node.name, class_to_bases):
            continue
        if _has_rule_suppression(source_lines, TRF004, node.lineno):
            continue

        assignments = _get_class_assignments(node)
        prefix_value = assignments.get("base_model_prefix")
        if prefix_value is None:
            continue
        if not (isinstance(prefix_value, ast.Constant) and isinstance(prefix_value.value, str)):
            violations.append(
                Violation(
                    file_path=file_path,
                    line_number=getattr(prefix_value, "lineno", node.lineno),
                    rule_id=TRF004,
                    message=f"{TRF004}: {node.name}.base_model_prefix should be a string literal.",
                )
            )
            continue
        if not prefix_value.value.strip() or any(char.isspace() for char in prefix_value.value):
            violations.append(
                Violation(
                    file_path=file_path,
                    line_number=getattr(prefix_value, "lineno", node.lineno),
                    rule_id=TRF004,
                    message=f"{TRF004}: {node.name}.base_model_prefix should be a non-empty canonical token.",
                )
            )

    return violations


def _function_argument_names(function_node: ast.FunctionDef) -> set[str]:
    names = {arg.arg for arg in function_node.args.args}
    names.update(arg.arg for arg in function_node.args.kwonlyargs)
    if function_node.args.vararg is not None:
        names.add(function_node.args.vararg.arg)
    if function_node.args.kwarg is not None:
        names.add(function_node.args.kwarg.arg)
    return names


def _function_uses_name(function_node: ast.FunctionDef, name: str) -> bool:
    return any(isinstance(node, ast.Name) and node.id == name and isinstance(node.ctx, ast.Load) for node in ast.walk(function_node))


def check_return_dict_usage(
    tree: ast.Module, file_path: Path, source_lines: list[str], violations: list[Violation]
) -> list[Violation]:
    class_to_bases = _collect_class_bases(tree)
    for node in tree.body:
        if not isinstance(node, ast.ClassDef):
            continue
        if not _inherits_pretrained_model(node.name, class_to_bases):
            continue
        if _has_rule_suppression(source_lines, TRF005, node.lineno):
            continue

        forward_method = _class_methods(node).get("forward")
        if forward_method is None:
            continue
        if "return_dict" not in _function_argument_names(forward_method):
            continue
        if _function_uses_name(forward_method, "return_dict"):
            continue

        violations.append(
            Violation(
                file_path=file_path,
                line_number=forward_method.lineno,
                rule_id=TRF005,
                message=f"{TRF005}: {node.name}.forward has return_dict argument but does not use it.",
            )
        )

    return violations


def check_gradient_checkpointing_contract(
    tree: ast.Module, file_path: Path, source_lines: list[str], violations: list[Violation]
) -> list[Violation]:
    class_to_bases = _collect_class_bases(tree)
    for node in tree.body:
        if not isinstance(node, ast.ClassDef):
            continue
        if not _inherits_pretrained_model(node.name, class_to_bases):
            continue
        if _has_rule_suppression(source_lines, TRF006, node.lineno):
            continue

        assignments = _get_class_assignments(node)
        supports_gc = assignments.get("supports_gradient_checkpointing")
        if not (isinstance(supports_gc, ast.Constant) and supports_gc.value is True):
            continue

        methods = _class_methods(node)
        if "_set_gradient_checkpointing" in methods:
            continue
        if "_gradient_checkpointing_func" in assignments:
            continue
        if any(isinstance(base, ast.Name) and base.id.endswith("PreTrainedModel") for base in node.bases):
            # Directly extending a model base often inherits the hook from the parent class in the same model family.
            continue

        violations.append(
            Violation(
                file_path=file_path,
                line_number=getattr(supports_gc, "lineno", node.lineno),
                rule_id=TRF006,
                message=(
                    f"{TRF006}: {node.name} sets supports_gradient_checkpointing=True without visible "
                    "_set_gradient_checkpointing/_gradient_checkpointing_func hook."
                ),
            )
        )

    return violations


def check_tie_weights_contract(
    tree: ast.Module, file_path: Path, source_lines: list[str], violations: list[Violation]
) -> list[Violation]:
    for node in tree.body:
        if not isinstance(node, ast.ClassDef):
            continue
        if _has_rule_suppression(source_lines, TRF007, node.lineno):
            continue

        tie_weights = _class_methods(node).get("tie_weights")
        if tie_weights is None:
            continue

        uses_output_helpers = any(
            is_self_method_call(sub_node, "get_output_embeddings")
            or is_self_method_call(sub_node, "set_output_embeddings")
            for sub_node in ast.walk(tie_weights)
        )
        delegates_to_super = any(is_super_method_call(sub_node, "tie_weights") for sub_node in ast.walk(tie_weights))
        if uses_output_helpers or delegates_to_super:
            continue

        violations.append(
            Violation(
                file_path=file_path,
                line_number=tie_weights.lineno,
                rule_id=TRF007,
                message=(
                    f"{TRF007}: {node.name}.tie_weights should use output embedding helpers "
                    "or delegate to super().tie_weights()."
                ),
            )
        )

    return violations


def check_no_split_modules_shape(
    tree: ast.Module, file_path: Path, source_lines: list[str], violations: list[Violation]
) -> list[Violation]:
    for node in tree.body:
        if not isinstance(node, ast.ClassDef):
            continue
        if _has_rule_suppression(source_lines, TRF008, node.lineno):
            continue

        assignments = _get_class_assignments(node)
        value = assignments.get("_no_split_modules")
        if value is None:
            continue
        if not isinstance(value, (ast.List, ast.Tuple)):
            violations.append(
                Violation(
                    file_path=file_path,
                    line_number=getattr(value, "lineno", node.lineno),
                    rule_id=TRF008,
                    message=f"{TRF008}: {node.name}._no_split_modules should be a list or tuple of strings.",
                )
            )
            continue

        if any(not isinstance(element, ast.Constant) or not isinstance(element.value, str) or not element.value for element in value.elts):
            violations.append(
                Violation(
                    file_path=file_path,
                    line_number=getattr(value, "lineno", node.lineno),
                    rule_id=TRF008,
                    message=f"{TRF008}: {node.name}._no_split_modules should contain non-empty strings only.",
                )
            )

    return violations


def check_generation_mixin_hooks(
    tree: ast.Module, file_path: Path, source_lines: list[str], violations: list[Violation]
) -> list[Violation]:
    local_classes = {node.name: node for node in tree.body if isinstance(node, ast.ClassDef)}

    def local_base_has_generation_hook(class_name: str, visiting: set[str] | None = None) -> bool:
        if visiting is None:
            visiting = set()
        if class_name in visiting or class_name not in local_classes:
            return False
        visiting.add(class_name)

        class_node = local_classes[class_name]
        methods = _class_methods(class_node)
        if "prepare_inputs_for_generation" in methods or "_reorder_cache" in methods or "can_generate" in methods:
            return True

        for base in class_node.bases:
            if not isinstance(base, (ast.Name, ast.Attribute)):
                continue
            try:
                base_name = _simple_name(full_name(base))
            except ValueError:
                continue
            if base_name == "GenerationMixin":
                continue
            if local_base_has_generation_hook(base_name, visiting):
                return True
        return False

    for node in tree.body:
        if not isinstance(node, ast.ClassDef):
            continue
        if _has_rule_suppression(source_lines, TRF009, node.lineno):
            continue

        direct_generation_mixin = False
        for base in node.bases:
            if not isinstance(base, (ast.Name, ast.Attribute)):
                continue
            try:
                if _simple_name(full_name(base)) == "GenerationMixin":
                    direct_generation_mixin = True
                    break
            except ValueError:
                continue
        if not direct_generation_mixin:
            continue

        methods = _class_methods(node)
        if "prepare_inputs_for_generation" in methods or "_reorder_cache" in methods:
            continue
        if "can_generate" in methods:
            continue

        non_generation_bases: list[str] = []
        has_external_non_generation_base = False
        for base in node.bases:
            if not isinstance(base, (ast.Name, ast.Attribute)):
                continue
            try:
                base_name = _simple_name(full_name(base))
            except ValueError:
                has_external_non_generation_base = True
                continue
            if base_name == "GenerationMixin":
                continue
            non_generation_bases.append(base_name)
            if base_name not in local_classes:
                has_external_non_generation_base = True

        # Conservative behavior: if hooks may come from any other parent class,
        # do not emit a violation.
        if has_external_non_generation_base or non_generation_bases:
            continue

        if any(local_base_has_generation_hook(base_name) for base_name in non_generation_bases):
            continue

        violations.append(
            Violation(
                file_path=file_path,
                line_number=node.lineno,
                rule_id=TRF009,
                message=(
                    f"{TRF009}: {node.name} directly inherits GenerationMixin but does not define "
                    "prepare_inputs_for_generation/_reorder_cache/can_generate."
                ),
            )
        )

    return violations


def check_cache_argument_usage(
    tree: ast.Module, file_path: Path, source_lines: list[str], violations: list[Violation]
) -> list[Violation]:
    for node in tree.body:
        if not isinstance(node, ast.ClassDef):
            continue
        if _has_rule_suppression(source_lines, TRF010, node.lineno):
            continue

        forward_method = _class_methods(node).get("forward")
        if forward_method is None:
            continue

        arg_names = _function_argument_names(forward_method)
        if "past_key_values" not in arg_names:
            continue
        if "use_cache" in arg_names and _function_uses_name(forward_method, "use_cache"):
            continue
        if _function_uses_name(forward_method, "past_key_values"):
            continue

        violations.append(
            Violation(
                file_path=file_path,
                line_number=forward_method.lineno,
                rule_id=TRF010,
                message=(
                    f"{TRF010}: {node.name}.forward exposes past_key_values/use_cache but does not reference them."
                ),
            )
        )

    return violations


def check_post_init_order(
    tree: ast.Module, file_path: Path, source_lines: list[str], violations: list[Violation]
) -> list[Violation]:
    class_to_bases = _collect_class_bases(tree)
    for node in tree.body:
        if not isinstance(node, ast.ClassDef):
            continue
        if not _inherits_pretrained_model(node.name, class_to_bases):
            continue
        if _has_rule_suppression(source_lines, TRF011, node.lineno):
            continue

        init_method = _class_methods(node).get("__init__")
        if init_method is None:
            continue

        post_init_index = None
        for index, statement in enumerate(init_method.body):
            if isinstance(statement, ast.Expr) and is_self_method_call(statement.value, "post_init"):
                post_init_index = index
                break
        if post_init_index is None:
            continue

        trailing_statements = init_method.body[post_init_index + 1 :]
        has_trailing_self_assignments = any(
            isinstance(statement, (ast.Assign, ast.AnnAssign))
            and any(
                isinstance(target, ast.Attribute)
                and isinstance(target.value, ast.Name)
                and target.value.id == "self"
                for target in (statement.targets if isinstance(statement, ast.Assign) else [statement.target])
            )
            for statement in trailing_statements
        )
        if not has_trailing_self_assignments:
            continue

        violations.append(
            Violation(
                file_path=file_path,
                line_number=init_method.lineno,
                rule_id=TRF011,
                message=f"{TRF011}: {node.name} assigns self.* after self.post_init() in __init__.",
            )
        )

    return violations


def check_doc_decorator_usage(
    tree: ast.Module, file_path: Path, source_lines: list[str], violations: list[Violation]
) -> list[Violation]:
    class_to_bases = _collect_class_bases(tree)
    for node in tree.body:
        if not isinstance(node, ast.ClassDef):
            continue
        if not _inherits_pretrained_model(node.name, class_to_bases):
            continue
        if _has_rule_suppression(source_lines, TRF012, node.lineno):
            continue

        for decorator in node.decorator_list:
            if not (
                isinstance(decorator, ast.Call)
                and isinstance(decorator.func, (ast.Name, ast.Attribute))
                and _simple_name(full_name(decorator.func)) == "add_start_docstrings"
            ):
                continue
            has_non_empty_string_arg = any(
                isinstance(arg, ast.Constant) and isinstance(arg.value, str) and arg.value.strip()
                for arg in decorator.args
            )
            if has_non_empty_string_arg:
                continue

            violations.append(
                Violation(
                    file_path=file_path,
                    line_number=getattr(decorator, "lineno", node.lineno),
                    rule_id=TRF012,
                    message=f"{TRF012}: {node.name} uses add_start_docstrings without non-empty docstring arguments.",
                )
            )
            break

    return violations


def check_generated_modeling_banner(
    modular_file: Path, modeling_file: Path, source_lines: list[str]
) -> Violation | None:
    if _has_rule_suppression(source_lines, TRF002, 1):
        return None

    first_lines = source_lines[:8]
    if not any(BANNER_MARKERS[0] in line for line in first_lines):
        return Violation(
            file_path=modeling_file,
            line_number=1,
            rule_id=TRF002,
            message=(
                f"{TRF002}: {modeling_file.name} appears non-generated or banner diverged; "
                f"edit {modular_file.name} and run make fix-repo."
            ),
        )

    expected_modular_path = modular_file.as_posix()
    auto_gen_line = next((line for line in first_lines if BANNER_MARKERS[0] in line), "")
    if expected_modular_path not in auto_gen_line:
        return Violation(
            file_path=modeling_file,
            line_number=1,
            rule_id=TRF002,
            message=(
                f"{TRF002}: {modeling_file.name} appears non-generated or banner diverged; "
                f"edit {modular_file.name} and run make fix-repo."
            ),
        )

    if not any(BANNER_MARKERS[1] in line for line in first_lines):
        return Violation(
            file_path=modeling_file,
            line_number=1,
            rule_id=TRF002,
            message=(
                f"{TRF002}: {modeling_file.name} appears non-generated or banner diverged; "
                f"edit {modular_file.name} and run make fix-repo."
            ),
        )

    return None


def _is_modeling_candidate(path: Path) -> bool:
    return path.suffix == ".py" and path.name.startswith(("modeling_", "modular_")) and MODELS_ROOT in path.parents


def _git_diff(base_ref: str, triple_dot: bool) -> list[str]:
    diff_operator = "..." if triple_dot else ".."
    range_ref = f"{base_ref}{diff_operator}HEAD"
    command = ["git", "diff", "--name-only", "--diff-filter=ACMR", range_ref]
    result = subprocess.run(command, capture_output=True, text=True, check=False)
    if result.returncode != 0:
        return []
    return [line for line in result.stdout.splitlines() if line.strip()]


def get_changed_modeling_files(base_ref: str) -> set[Path]:
    changed_paths = _git_diff(base_ref, triple_dot=True)
    if not changed_paths:
        changed_paths = _git_diff(base_ref, triple_dot=False)

    filtered_paths: set[Path] = set()
    for path_str in changed_paths:
        path = Path(path_str)
        if _is_modeling_candidate(path):
            filtered_paths.add(path)
    return filtered_paths


def _trf002_pairs_for_paths(paths: set[Path] | None) -> set[tuple[Path, Path]]:
    pairs: set[tuple[Path, Path]] = set()

    if paths is None:
        for modular_file in MODELS_ROOT.rglob("modular_*.py"):
            name = modular_file.stem.replace("modular_", "", 1)
            modeling_file = modular_file.parent / f"modeling_{name}.py"
            if modeling_file.exists() and not _is_rule_allowlisted_for_file(TRF002, modular_file):
                pairs.add((modular_file, modeling_file))
        return pairs

    for path in paths:
        if path.name.startswith("modular_"):
            name = path.stem.replace("modular_", "", 1)
            modeling_file = path.parent / f"modeling_{name}.py"
            if modeling_file.exists() and not _is_rule_allowlisted_for_file(TRF002, path):
                pairs.add((path, modeling_file))
        elif path.name.startswith("modeling_"):
            name = path.stem.replace("modeling_", "", 1)
            modular_file = path.parent / f"modular_{name}.py"
            if modular_file.exists() and not _is_rule_allowlisted_for_file(TRF002, path):
                pairs.add((modular_file, path))
    return pairs


def analyze_file(file_path: Path, text: str, enabled_rules: set[str] | None = None) -> list[Violation]:
    if enabled_rules is None:
        enabled_rules = DEFAULT_ENABLED_TRF_RULES

    violations: list[Violation] = []
    source_lines = text.splitlines()
    tree = ast.parse(text, filename=str(file_path))

    for node in ast.walk(tree):
        violations = check_init_weights(node, violations, file_path)
        violations = check_post_init(node, violations, file_path)
    if TRF001 in enabled_rules:
        violations = check_pretrainedmodel_parent_init(tree, file_path, source_lines, violations)
    if TRF003 in enabled_rules:
        violations = check_config_class_consistency(tree, file_path, source_lines, violations)
    if TRF004 in enabled_rules:
        violations = check_base_model_prefix(tree, file_path, source_lines, violations)
    if TRF005 in enabled_rules:
        violations = check_return_dict_usage(tree, file_path, source_lines, violations)
    if TRF006 in enabled_rules:
        violations = check_gradient_checkpointing_contract(tree, file_path, source_lines, violations)
    if TRF007 in enabled_rules:
        violations = check_tie_weights_contract(tree, file_path, source_lines, violations)
    if TRF008 in enabled_rules:
        violations = check_no_split_modules_shape(tree, file_path, source_lines, violations)
    if TRF009 in enabled_rules:
        violations = check_generation_mixin_hooks(tree, file_path, source_lines, violations)
    if TRF010 in enabled_rules:
        violations = check_cache_argument_usage(tree, file_path, source_lines, violations)
    if TRF011 in enabled_rules:
        violations = check_post_init_order(tree, file_path, source_lines, violations)
    if TRF012 in enabled_rules:
        violations = check_doc_decorator_usage(tree, file_path, source_lines, violations)

    return [
        violation
        for violation in violations
        if not (violation.rule_id is not None and _is_rule_allowlisted_for_file(violation.rule_id, violation.file_path))
    ]


def format_violation(violation: Violation) -> str:
    return colored_error_message(str(violation.file_path), violation.line_number, violation.message)


def emit_violation(violation: Violation, github_annotations: bool):
    if github_annotations:
        print(
            f"::error file={violation.file_path},line={violation.line_number}::{violation.message}",
            file=sys.stderr,
        )
        return

    print(format_violation(violation), file=sys.stderr)


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--changed-only",
        action="store_true",
        help="Only check changed modeling/modular files compared to --base-ref.",
    )
    parser.add_argument(
        "--base-ref",
        default="origin/main",
        help="Base git ref used with --changed-only (default: origin/main).",
    )
    parser.add_argument(
        "--github-annotations",
        action="store_true",
        help="Emit GitHub Actions annotation format output.",
    )
    parser.add_argument(
        "--no-progress",
        action="store_true",
        help="Disable interactive progress animation.",
    )
    parser.add_argument(
        "--enable-all-trf-rules",
        action="store_true",
        help="Enable TRF001-TRF012 (defaults already enable all).",
    )
    parser.add_argument(
        "--enable-rules",
        default="",
        help="Comma-separated TRF rule ids to enable in addition to defaults (e.g. TRF003,TRF004).",
    )
    parser.add_argument(
        "--list-rules",
        action="store_true",
        help="List available TRF rules and exit.",
    )
    parser.add_argument(
        "--rule",
        default="",
        help="Show detailed docs for one rule id (e.g. TRF001) and exit.",
    )
    return parser.parse_args()


def should_show_progress(args: argparse.Namespace) -> bool:
    return (not args.no_progress) and (not args.github_annotations) and sys.stderr.isatty()


def resolve_enabled_rules(args: argparse.Namespace) -> set[str]:
    if args.enable_all_trf_rules:
        return _validate_rule_ids(set(TRF_RULES))

    enabled_rules = set(DEFAULT_ENABLED_TRF_RULES)
    if args.enable_rules.strip():
        enabled_rules.update(rule_id.strip() for rule_id in args.enable_rules.split(",") if rule_id.strip())
    return _validate_rule_ids(enabled_rules)


def format_rule_summary(rule_id: str) -> str:
    spec = TRF_RULE_SPECS[rule_id]
    default_label = "enabled" if spec["default_enabled"] else "disabled"
    return f"{rule_id}: {spec['description']} (default: {default_label})"


def format_rule_details(rule_id: str) -> str:
    spec = TRF_RULE_SPECS[rule_id]
    explanation = spec["explanation"]
    default_label = "yes" if spec["default_enabled"] else "no"
    return "\n".join(
        [
            rule_id,
            "",
            f"Summary: {spec['description']}",
            f"Default enabled: {default_label}",
            "",
            "What it does",
            "",
            explanation["what_it_does"],
            "",
            "Why is this bad?",
            "",
            explanation["why_bad"],
            "",
            "Example",
            "",
            explanation["bad_example"],
            "",
            "Use instead:",
            "",
            explanation["good_example"],
        ]
    )


def maybe_handle_rule_docs_cli(args: argparse.Namespace) -> bool:
    if args.list_rules:
        for rule_id in sorted(TRF_RULE_SPECS):
            print(format_rule_summary(rule_id))
        return True

    if args.rule:
        rule_id = args.rule.strip().upper()
        _validate_rule_ids({rule_id})
        print(format_rule_details(rule_id))
        return True

    return False


def main():
    args = parse_args()
    if maybe_handle_rule_docs_cli(args):
        return

    violations: list[Violation] = []
    enabled_rules = resolve_enabled_rules(args)
    selected_paths = get_changed_modeling_files(args.base_ref) if args.changed_only else None

    modeling_files = list(iter_modeling_files(selected_paths))
    trf002_pairs = sorted(_trf002_pairs_for_paths(selected_paths)) if TRF002 in enabled_rules else []

    show_progress = should_show_progress(args)
    status_ctx = (
        CONSOLE.status(
            f"[bold blue]Checking modeling structure ({len(modeling_files)} files, {len(trf002_pairs)} banner pairs)...[/bold blue]"
        )
        if show_progress
        else nullcontext()
    )

    with status_ctx:
        for file_path in modeling_files:
            try:
                text = file_path.read_text(encoding="utf-8")
                violations.extend(analyze_file(file_path, text, enabled_rules=enabled_rules))
            except Exception as exc:
                violations.append(Violation(file_path=file_path, line_number=1, message=f"failed to parse ({exc})."))

        for modular_file, modeling_file in trf002_pairs:
            try:
                text = modeling_file.read_text(encoding="utf-8")
            except Exception as exc:
                violations.append(
                    Violation(file_path=modeling_file, line_number=1, message=f"failed to parse ({exc}).")
                )
                continue

            banner_violation = check_generated_modeling_banner(modular_file, modeling_file, text.splitlines())
            if banner_violation is not None:
                violations.append(banner_violation)

    if len(violations) > 0:
        violations = sorted(violations, key=lambda v: (str(v.file_path), v.line_number, v.message))
        for violation in violations:
            emit_violation(violation, github_annotations=args.github_annotations)
        raise ValueError("Some errors in modelings. Check the above message")

    print("OK")


if __name__ == "__main__":
    main()
